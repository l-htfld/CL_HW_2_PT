{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d26632-eec8-4154-a6d1-fa2aa5978a4f",
   "metadata": {},
   "source": [
    "## Инструменты NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d7ac65-fa05-4b58-a17a-aa2c0e5de7d1",
   "metadata": {},
   "source": [
    "Вспомним понятие **пайплайн**: это набор действий, которые нужно выполнить, чтобы решить какую-нибудь задачу (причем обычно данные передаются с этапа на этап напрямую). \n",
    "\n",
    "Классический пайплайн для многих задач NLP содержит примерно похожие шаги. Как правило, нам нужно:\n",
    "\n",
    "- собрать текстовые данные\n",
    "- очистить их (например, удалить хештеги и ссылки - если они мешают при решении задачи)\n",
    "- разделить тексты на предложения и на токены (тоже зависит от задачи - современные нейронные сети ожидают от нас предложений, внутри которых токены)\n",
    "\n",
    "Дальнейшие шаги более опциональны:\n",
    "\n",
    "- нормализация (мы можем хотеть привести все к одному регистру или к словарным формам)\n",
    "- морфологическая разметка (иногда нужна информация о частях речи и проч.)\n",
    "- синтаксическая разметка (еще более редкий шаг)\n",
    "- семантическая разметка и другие виды разметки\n",
    "\n",
    "Как можно заметить, для создания лингвистического корпуса необходимо выполнить все перечисленные шаги + проиндексировать размеченные данные, т.е. сложить их в специальную базу, по которой можно быстро искать нужное. \n",
    "\n",
    "В основном на сегодняшний день современные решения NLP-задач подразумевают только самые базовые шаги, а более продвинутые даже могут быть вредными (как нормализация). Но для всех этих этапов пайплайна есть инструменты в питоне, и многие из них можно использовать и для теоретических исследований, в отрыве от решения задач компьютерной лингвистики. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb3bd9-a0a7-4907-84d1-0626a07ccae7",
   "metadata": {},
   "source": [
    "### Библиотеки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a7d3e-3209-4e16-831e-6645a893d349",
   "metadata": {},
   "source": [
    "В NLP существует несколько крупных библиотек, в которых обычно содержатся все необходимые базовые инструменты обработки данных, а также часто и что-нибудь сверх того. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145515de-7114-4e98-802d-f7dd9d34aefa",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### NLTK\n",
    "\n",
    "Самая старая и известная такая библиотека - это [NLTK](https://www.nltk.org/). Она существует уже много лет и была написана еще под питон версии 2.0 (мы с вами работаем в третьей), так что это библиотека очень разросшаяся, в ней есть самые разные не всегда связанные друг с другом модули, и даже не все из них обновлены под питон 3.0. \n",
    "\n",
    "Это внешняя библиотека, то есть, ее необходимо устанавливать. В командной строке от имени администратора наберите ```pip install nltk```, после чего нужно запустить питон в интерактивном режиме и один раз выполнить следующий код:\n",
    "\n",
    "```\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "```\n",
    "\n",
    "Если вы работаете в колабе, nltk там, скорее всего, уже установлен, но может все равно понадобиться загружать дополнительные модули ('all' можно не загружать, достаточно конкретно того, что вам нужно, например, для токенизации нужно ```nltk.download('punct')```. \n",
    "\n",
    "NLTK - больше исследовательская библиотека, конструктор своего рода. Для NLTK есть учебник, написанный авторами: [NLTK book](https://www.nltk.org/book/). Для этого учебника специально существует подмодуль book (его нужно загрузить), который обычно импортируется целиком:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2709fce-461d-4c75-94df-f2bc30cf92b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f7be8d-f72c-4774-b545-0f3dabda06bd",
   "metadata": {},
   "source": [
    "В этом модуле есть некий набор текстов и набор предложений, с которыми можно поиграться. \n",
    "\n",
    "Центральный объект для NLTK (по крайней мере, при работе с корпусами) - это Text (nltk.text.Text). По сути, в этом объекте содержится сам текст в виде списка токенов, но у него есть дополнительные методы. Что можно делать с объектом класса Text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ee69eb-84a6-438f-9d4b-06d4587ca661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 84 matches:\n",
      "[ Moby Dick by Herman Melville 1851 ] ETYMOLOGY . ( Su\n",
      "hat white whale must be the same that some call Moby Dick .\" \" Moby Dick ?\" shouted Ahab . \" Do ye k\n",
      " must be the same that some call Moby Dick .\" \" Moby Dick ?\" shouted Ahab . \" Do ye know the white w\n",
      "ib in a squall . Death and devils ! men , it is Moby Dick ye have seen -- Moby Dick -- Moby Dick !\" \n",
      " devils ! men , it is Moby Dick ye have seen -- Moby Dick -- Moby Dick !\" \" Captain Ahab ,\" said Sta\n"
     ]
    }
   ],
   "source": [
    "text1.concordance('Moby', width=100, lines=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a76f70-ff65-485b-9967-bf41e0f81085",
   "metadata": {},
   "source": [
    "Конкорданс ищет первые n вхождений заданного слова с шириной контекста width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5b66f8a-ffec-4f4c-9eaf-74fef6e8cd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship boat sea time captain world man deck pequod other whales air\n",
      "water head crew line thing side way body\n"
     ]
    }
   ],
   "source": [
    "text1.similar('whale', num=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8501e5a3-76db-44a1-a348-7d3a8fc29666",
   "metadata": {},
   "source": [
    "similar возвращает num слов, которые встречаются в похожих контекстах (дистрибутивная похожесть). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e740f8-e89b-4fa5-89be-1f113356d490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_s the_and the_is the_in the_the the_as the_was the_which the_i\n",
      "a_in the_has the_when the_had the_with the_to the_by the_so the_that\n",
      "the_would the_a\n"
     ]
    }
   ],
   "source": [
    "text1.common_contexts(['whale', 'ship'])  # тоже можно задать параметр num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302818a9-2680-41d9-bd07-620fd07b6840",
   "metadata": {},
   "source": [
    "common_contexts ищет те самые совпадающие контексты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b3dc4bc-2309-4651-8c17-f23654e13e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXn0lEQVR4nO3debhlVX3m8e9LFValRRmEGFSsK86ABqE0zlVqYkdjNN2NinGOT4xG7bbTauPwSPl07EjyqImzqKTUYNRETZw6SjtPDIWigIKiQoMjOKAYB5Rf/7HXsXZd7r3r3qo7cOt+P89znjpnrbXXXmvvfc979z67zk1VIUnSXPZZ6QFIkq77DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFlqVktwryYWL0M/FSX53D5Z/ZJIP7uk4FstibZfdWG8ludVyr1fLx7DQstjTN+XpquoTVXXbxepvJkm2J/lFkh+3x3lJ/irJ/qNxnFpV91/KcSzEUm2XJFMtEK5qj4uTnLAb/TwuyScXe3xaeoaFNLe/rqobAIcAjwfuCnwqyfVXakBJ1q3UuoEDqmo/4BHA85P8/gqORcvIsNCKSrJPkhOSfDXJ95K8PclBre7VSd4xantSkg9lsDXJZaO6w5K8M8nlrZ9XtPJbJvlwK7siyalJDljoOKvqZ1V1FvBg4EYMwbHLb8ptXC9N8t0kP0pybpKjWt32JK9Jclo7S/lYkk2j8d+u1X0/yYVJHjaq2962xfuT/AS4T5IHJvli6+sbSZ7R2k7fLrdP8tEkP0xyfpIHT+v3lUne1/o5I8kt57k9PgOcDxw1vS7J/kne1PbFJUme1/bz7YHXAHdrZyc/nPcO0IozLLTSngb8EbAFuAnwA+CVre5/AHdob8j3Ap4APLamfUdN+037vcAlwBRwU+Ctk2rgr1rftwcOA7bt7mCr6sfAacC9Zqi+P3Bv4DbA/sDDgO+N6h8J/C/gYOAc4NQ2/uu3Pt8C/CZwPPCqJEeMlv1j4IXADYBPAm8A/qyd9RwFfHj6YJLsC7wH+GDr92nAqUnGl6mOB14AHAhc1NYxpxaK9wCOBD43Q5OXt/kfzrBfHwM8vqq+BDwJ+ExV7VdVB/TWpesOw0Ir7UnAc6vqsqr6OcMb+XFJ1lfVvwOPBl4C/APwtKq6bIY+7sIQBs+sqp+0s4BPAlTVRVV1WlX9vKoub31t2cMxfxM4aIbyqxnezG8HpKq+VFXfGtW/r6o+3ub5XIbfsA8DHgRcXFV/X1W/rKrPAe8AHjpa9l+r6lNVdU1V/ayt64gkN6yqH1TVZ2cYz12B/YAXVdUvqurDDKH6iFGbd1XVmVX1S4bwOroz9yuA7wOvB06oqg+NK1twHw88u6p+XFUXAy9m2I9axQwLrbRNwLvaZZIfAl8CfgXcGKCqzgC+xnCG8PZZ+jgMuKS94e0iyY2TvLVdqvkRQ+gcvIdjvinDG+Yu2pvxKxjOjL6b5OQkNxw1uXTU9qrWx00YtsHvTLZB2w6PBH5rpmWb/wI8ELikXdK62wzjvAlwaVVdMyq7pI1/4tuj5//OEC5zObiqDqyq21fVy2aqB/Zt65ltnVqFDAuttEuBB1TVAaPHxqr6BkCSpwAbGH6bf9Ycfdw8yfoZ6v43UMAdquqGwKMYgme3JNkP+F3gEzPVV9XLqupY4AiGy1HPHFUfNq2fgxjmdSnwsWnbYL+qevK462nrOauqHsJweelfmDlIvwkclmT8c35z4BvzmuzuuYLhrGfTqGy8Tr/mepUyLLSc9k2ycfRYz/CB5wsnH/YmOSTJQ9rz2wB/yfAG/2jgWUmOnqHfM4FvAS9Kcv3W9z1a3Q2Aq4Ark9yUXd+85y3JhiTHMrwx/wD4+xna3DnJ77TPCn4C/AwY/1b/wCT3THI9hs8uTq+qSxkuDd0myaOT7Nsed24fCM80lutl+P8d+1fV1cCPpq1n4gyGs4VntT63An/Izs9zFl1V/YohuF6Y5AZtv/4FwxkdwHeAm7VtoFXEsNByej/w09FjG/B3wLuBDyb5MXA6wyWZ9QxvMCdV1eer6ivAc4A3J9kw7rS9Qf0hcCvg/wGXAQ9v1S8AjgGuBN4HvHOBY35WG9f3gDcBZwN3r6qfzND2hsDrGMLkkrbM34zq3wKcyHD56ViGEJx8aH5/hmv932S4NHQSwxnVbB4NXNwurT2J4bLVLqrqFwzb5QEMv/G/CnhMVV0wn4nvgacxhOXXGD6MfwtwSqv7MMNdVN9OcsUSj0OLKP7xI2npJdkOXFZVz1vpsUi7wzMLSVKXYSFJ6vIylCSpyzMLSVLXTPelr3oHH3xwTU1NrfQwJGlVOfvss6+oqkNmqtsrw2JqaoodO3as9DAkaVVJcslsdV6GkiR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSupY0LBL+KKESbtdeb0147wL7+GjC5qUZ4eoxNQXbtu1atn49JNduu20bHHAAbN268/XWrUPZ5PVMtm0b1jM1BfvsMzympoblpqbmHt9knZNlprdPhsekr8mYJrZu3TmurVuHtuOysampoW7jxuEx3caNw/KTsY/nPt4m69fvHMv0Mc+2jRZq69Zrz3Ost10nxttrsn3majPfPvfEZJvOdw6w6/ZfyDLjY3effXYeR71jc7ye9euv/TMw2ZbjdpPjbnwcjde7fv1QNzm2Nm7c9ZiemhqW27hxWGbbtp39T35GDjhgZ//77LOzz+mP9et3HkPjNpP+x+0m/S7kGFiIVNXS9AwkvA24CfDhKk5M2Ao8o4oHLaCPj7Zldsx3mc2bN9eOHfNuvipM3hzGu2umsnH5pG6m1zPt9pnegMbmOlRmWnamsc7WZjyX6W3nmt986sftJnOfz3gW40dj+j6a3u981zPT9plt3vMd957OcbwtF7rOhS4D89tvs61vpn5m+tkYt1tsc41/Kda1O5KcXVUz/nK+ZGcWCfsB9wSeABw/qtov4Z8TLkg4NSGt/fMTzko4L+HkSXnz6IRzWt1dlmrMkqSZLeVlqIcA/1bFl4HvJRzbyu8EPB04AjgcuEcrf0UVd67iKOA3YJezj/9QxdHAnwOnzLSyJE9MsiPJjssvv3zRJyNJa9lShsUjgLe2529trwHOrOKyKq4BzgGmWvl9Es5IOBe4L3DkqK9/BKji48ANEw6YvrKqOrmqNlfV5kMOOWSx5yJJa9r6peg04SCGN/w7JBSwDijgfcDPR01/BaxP2Ai8CthcxaUJ24DxR5fTr8At3QctkqRrWaozi+OAN1exqYqpKg4Dvg7ca5b2k2C4on3Wcdy0+ocDJNwTuLKKK5di0NdlmzbBiSfuWrZu3cxtTzwR9t8ftmzZ+XrLlqFs8nq25TZtGh6Tu5c2bRqW27Rp7vFN1jlZZrb2k74mY5rYsmXnuCbl47KxTZuGug0bhsd0k7LJ2MdzH2+Tdet2jmX6mGfbRgu1Zcu15zl9LvMx3l7T+5ipzXz73BOTbTrfOcCu238hy4yP3WTncdQ7NsfrWbfu2j8DM41nctyNj6PxetetG+omx9aGDbse05PxbNgwLHPiiTv7n/yM7L//zv6TnX1Of6xbt/MYGreZ9D9uN+l3IcfAQizJ3VAJHwFOquLfRmX/FXgy8NXJ3VAJrwB2VLE94S8ZLlV9G/gycEkV29rdUOcAW4B9gT+p4sy51r833g0lSUttrruhlvTW2ZViWEjSwq3IrbOSpL2HYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1NUNi4Sr9qR+sSVsTzhuKdexbdvO51NTu77ek74Wu37r1vn1sRrMNofp5ZM5a8/sDcfMdFNTKz2CvVuqau4G4aoq9tvd+sWWsB14bxX/PFubzZs3144dO/ZkHUw2SzL829lM8+prsesndb0+VoPZ5jC9fG+Y63XB3rgd98Y5LbckZ1fV5pnq5n0ZKuHQhI8nnJNwXsK9RnUvTPh8wukJN25l2xNe3cq+lrA14ZSEL7U3/Mmyr07YkXB+wgtG5ccmfCzh7IQPJBy6W7OXJO2xhXxm8cfAB6o4Gvht4JxWfn3g9Cp+G/g48KejZQ4E7gb8d+DdwEuBI4E7JBzd2jy3is3AHYEtCXdM2Bd4OXBcFccCpwAvnGtwSZ6YZEeSHZdffvkCpiVJ6lm/gLZnAae0N/J/qfp1WPwCeG97fjbwe6Nl3lNFJZwLfKeKcwESzgemGALnYQlPbGM5FDgCuAY4CjitXQZaB3xrrsFV1cnAyTBchlrAvCRJHfMOiyo+nnBv4A+A7QkvqeJNwNVVTN6cfzWtz5+3f68ZPZ+8Xp9wC+AZwJ2r+EG7PLURCHB+FXfbnUlJkhbXQj6z2MRwdvA64PXAMYuw/hsCPwGubJ91PKCVXwgckgxhkbBvwpGLsL55OfHEnc83bdr19Z70tdj1W7bMr4/VYLY5TC+fzFl7Zm84ZqbbtGmlR7B3m/fdUAmPBZ4JXA1cBTymiq+P74Zqt7Q+qIrHje9aSphqz49q7cZ124G7A5cCVwLvrmJ7+0zjZcD+DGcrf1vF65bjbihJWovmuhuqGxarkWEhSQu3KLfOSpLWLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6UlUrPYZFl+Ry4JJ5ND0YuGKJh3Ndspbmu5bmCmtrvs516WyqqkNmqtgrw2K+kuyoqs0rPY7lspbmu5bmCmtrvs51ZXgZSpLUZVhIkrrWelicvNIDWGZrab5raa6wtubrXFfAmv7MQpI0P2v9zEKSNA+GhSSpa82GRZLfT3JhkouSnLDS41mIJBcnOTfJOUl2tLKDkpyW5Cvt3wNbeZK8rM3zC0mOGfXz2Nb+K0keOyo/tvV/UVs2yzi3U5J8N8l5o7Iln9ts61ih+W5L8o22f89J8sBR3bPb2C9M8h9H5TMez0lukeSMVv62JNdr5Rva64ta/dQyzPWwJB9J8sUk5yf5b618r9u/c8x19e7bqlpzD2Ad8FXgcOB6wOeBI1Z6XAsY/8XAwdPK/ho4oT0/ATipPX8g8H+AAHcFzmjlBwFfa/8e2J4f2OrObG3Tln3AMs7t3sAxwHnLObfZ1rFC890GPGOGtke0Y3UDcIt2DK+b63gG3g4c356/Bnhye/7nwGva8+OBty3DXA8FjmnPbwB8uc1pr9u/c8x11e7bZXkDuK49gLsBHxi9fjbw7JUe1wLGfzHXDosLgUPb80OBC9vz1wKPmN4OeATw2lH5a1vZocAFo/Jd2i3T/KbY9c1zyec22zpWaL6zvaHscpwCH2jH8ozHc3vDvAJY38p/3W6ybHu+vrXLMu/nfwV+b2/fv9Pmumr37Vq9DHVT4NLR68ta2WpRwAeTnJ3kia3sxlX1rfb828CN2/PZ5jpX+WUzlK+k5ZjbbOtYKU9tl15OGV0yWeh8bwT8sKp+Oa18l75a/ZWt/bJol0buBJzBXr5/p80VVum+Xathsdrds6qOAR4APCXJvceVNfxKsVfeE70cc7sObL9XA7cEjga+Bbx4Bcey6JLsB7wDeHpV/Whct7ft3xnmumr37VoNi28Ah41e36yVrQpV9Y3273eBdwF3Ab6T5FCA9u93W/PZ5jpX+c1mKF9JyzG32dax7KrqO1X1q6q6Bngdw/6Fhc/3e8ABSdZPK9+lr1a/f2u/pJLsy/DmeWpVvbMV75X7d6a5ruZ9u1bD4izg1u1ugusxfAj07hUe07wkuX6SG0yeA/cHzmMY/+SukMcyXCOllT+m3VlyV+DKdjr+AeD+SQ5sp8L3Z7jm+S3gR0nu2u4kecyor5WyHHObbR3LbvKm1vwnhv0LwxiPb3e73AK4NcMHujMez+036I8Ax7Xlp2+7yXyPAz7c2i+Zts3fAHypql4yqtrr9u9sc13V+3Y5P+S5Lj0Y7rT4MsOdBs9d6fEsYNyHM9wR8Xng/MnYGa5Jfgj4CvB/gYNaeYBXtnmeC2we9fUnwEXt8fhR+eZ2EH8VeAXL+MEn8I8Mp+dXM1yHfcJyzG22dazQfN/c5vMFhh/8Q0ftn9vGfiGju9RmO57b8XJm2w7/BGxo5Rvb64ta/eHLMNd7Mlz++QJwTns8cG/cv3PMddXuW7/uQ5LUtVYvQ0mSFsCwkCR1GRaSpC7DQpLUZVhIkroMC61ZSV6a5Omj1x9I8vrR6xcn+Yvd7HtrkvfOUnfPJGcmuaA9njiqO6R9U+jnktwryUOTfCnJR3ZjDM/ZnbFLMzEstJZ9Crg7QJJ9gIOBI0f1dwc+PZ+OkqybZ7vfAt4CPKmqbsdwP/6fJfmD1uR+wLlVdaeq+gTD/7v406q6z3z6n8aw0KIxLLSWfZrh2zphCInzgB+3/xm8Abg98Nkk92u/6Z/bvvxtA/z674qclOSzwEMz/N2BC9rr/zzLOp8CbK+qzwJU1RXAs4ATkhzN8FXaD8nwtw5OZAiTNyT5myRHtjOSc9oX0d26jeNRo/LXJlmX5EXAb7SyUxd/02mtWd9vIu2dquqbSX6Z5OYMZxGfYfjGzrsxfFPnuQy/UG0H7ldVX07yJuDJwN+2br5XVcck2cjwv4Pvy/A/Z982y2qPBN44rWwHcGRVnZPk+Qz/U/mpAEnuw/CV1juSvBz4u6o6tX31w7oktwceDtyjqq5O8irgkVV1QpKnVtXRe7aVpIFnFlrrPs0QFJOw+Mzo9aeA2wJfr6ovt/ZvZPiDRROTULhda/eVGr4W4R+WYKyfAZ6T5H8Cm6rqpwyXrY4FzkpyTnt9+BKsW2ucYaG1bvK5xR0YLkOdznBmMd/PK36ywPV9keHNfexYhu/5mlNVvQV4MPBT4P1J7svw/UlvrKqj2+O2VbVtgWOSugwLrXWfBh4EfL+Gr47+PnAAQ2B8muFL3aaS3Kq1fzTwsRn6uaC1u2V7/YhZ1vdK4HHt8wmS3Ag4ieGzijklORz4WlW9jOEbRu/I8OV4xyX5zdbmoCSb2iJXZ/iabGmPGRZa685luAvq9GllV1bVFVX1M+DxwD8lORe4huHvHe+itXsi8L72AfeMfy+hhq/RfhTwuiQXMATSKVX1nnmM9WHAee1y01HAm6rqi8DzGP5y4heA0xj+bCjAycAX/IBbi8FvnZUkdXlmIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSuv4/co4BGDULVA0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text1.dispersion_plot(['Ahab', 'Ishmael'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc72c45-9afa-4e2e-935b-de4236ca2fbb",
   "metadata": {},
   "source": [
    "Можно построить график распределения слов по тексту (без NumPy и matplotlib график не работает, поэтому установите эти две библиотеки, если еще не). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95e13fe4-7ce9-4aca-82bd-57c9cb84cfee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.count('Moby')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a4574-6ee6-463d-b5a2-4ff98f67f5f6",
   "metadata": {},
   "source": [
    "Можно посчитать количество вхождений какого-то слова. Кстати, к текстам можно применять обычные функции len(), set() и подобные. И срезы с индексами работают!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09e07e98-6732-490d-b6bd-64af937f3f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building ngram index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long , from one to the top - mast , and no coffin and went out a sea\n",
      "captain -- this peaking of the whales . , so as to preserve all his\n",
      "might had in former years abounding with them , they toil with their\n",
      "lances , strange tales of Southern whaling . at once the bravest\n",
      "Indians he was , after in vain strove to pierce the profundity . ?\n",
      "then ?\" a levelled flame of pale , And give no chance , watch him ;\n",
      "though the line , it is to be gainsaid . have been\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'long , from one to the top - mast , and no coffin and went out a sea\\ncaptain -- this peaking of the whales . , so as to preserve all his\\nmight had in former years abounding with them , they toil with their\\nlances , strange tales of Southern whaling . at once the bravest\\nIndians he was , after in vain strove to pierce the profundity . ?\\nthen ?\" a levelled flame of pale , And give no chance , watch him ;\\nthough the line , it is to be gainsaid . have been'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.generate(length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4002bce0-0f6a-469c-b8c0-86fc93408382",
   "metadata": {},
   "source": [
    "Можно сгенерировать текст, \"похожий\" на оригинальный. Это делается с помощью n-грамов (или n-грамм, я видела разные варианты по-русски...): nltk просто в случайном порядке совмещает эти n-грамы. Как можно видеть, не слишком полезный метод, однако можно побаловаться. \n",
    "\n",
    "Важнее то, что в nltk есть утилиты для работы с n-грамами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6793c0a5-f634-40e3-8d80-1ffb56988bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('THE', 'suburb', 'of'),\n",
       " ('suburb', 'of', 'Saffron'),\n",
       " ('of', 'Saffron', 'Park'),\n",
       " ('Saffron', 'Park', 'lay'),\n",
       " ('Park', 'lay', 'on'),\n",
       " ('lay', 'on', 'the'),\n",
       " ('on', 'the', 'sunset'),\n",
       " ('the', 'sunset', 'side'),\n",
       " ('sunset', 'side', 'of'),\n",
       " ('side', 'of', 'London'),\n",
       " ('of', 'London', ','),\n",
       " ('London', ',', 'as'),\n",
       " (',', 'as', 'red'),\n",
       " ('as', 'red', 'and'),\n",
       " ('red', 'and', 'ragged'),\n",
       " ('and', 'ragged', 'as'),\n",
       " ('ragged', 'as', 'a'),\n",
       " ('as', 'a', 'cloud'),\n",
       " ('a', 'cloud', 'of'),\n",
       " ('cloud', 'of', 'sunset'),\n",
       " ('of', 'sunset', '.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams # bigrams\n",
    "\n",
    "list(ngrams(sent9, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300580a8-c62c-4ce1-bb87-2b1ae8de4832",
   "metadata": {},
   "source": [
    "Функция ngrams (или bigrams) возвращает список всех н-грамов списка токенов, который ей дать. N-грамы еще принимают число n. \n",
    "\n",
    "У класса Text есть метод, который возвращает коллокации (частотные н-грамы):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd23a273-71cc-460a-9114-3aae9f7700f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n",
      "whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n",
      "years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n",
      "mate; white whale; ivory leg; one hand\n"
     ]
    }
   ],
   "source": [
    "text1.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25ec79d-4e05-4107-aa3b-e17167c2bfba",
   "metadata": {},
   "source": [
    "Как создать собственный объект класса Text? Достаточно токенизировать свой текст (любым токенизатором) и передать его в класс:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad3465-0269-489f-8ad2-f0d83259d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1100a81-c688-43c4-a646-30fdf873918d",
   "metadata": {},
   "source": [
    "Гораздо чаще на практике, однако, используются какие-то отдельные инструменты NLTK, предназначенные для обработки текста. Часть мы уже знаем: это токенизаторы и сентенайзер. \n",
    "\n",
    "Следующий шаг при обработке текстов - это обычно приведение слов к словарной форме. Это уже не такая простая задача, с помощью регулярок ее не решить, приходится использовать либо словари, либо более сложные методы (нейронные сети...)\n",
    "\n",
    "Еще в прошлом веке, правда, придумали (конечно же, для английского языка) более простой способ хоть как-то унифицировать разные словоформы одного слова: стемминг. \n",
    "\n",
    "Стемминг – это уже чисто историческое, можно сказать, явление: в 1980-х, когда еще не было даже графического интерфейса у компьютеров и тем более средств автоматического морфоразбора, Мартин Портер разработал свой алгоритм стемминга: усечение окончания от псевдоосновы. Этот алгоритм так и называется \"стеммер Портера\" и доступен в версиях для нескольких европейских языков, в т.ч. для русского (Snowball – чуть более новая версия). Алгоритм с помощью правил отсекает окончания и суффиксы, основываясь на особенностях языка. Как все правиловое, работает не без ошибок.\n",
    "\n",
    "В NLTK есть несколько стеммеров, а именно:\n",
    "\n",
    "1. PorterStemmer\n",
    "2. SnowballStemmer\n",
    "3. LancasterStemmer\n",
    "4. RegexpStemmer\n",
    "5. RSLPStemmer\n",
    "\n",
    "Все смотреть не будем, можете сами поискать, если интересно, нам хватит двух. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3121f4ea-85bd-4c39-8155-463b778237f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "example_words = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]\n",
    "for w in example_words:\n",
    " print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbdf0342-2e57-4c84-a3a2-8f3dcdcac87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пердикк\n",
      "не\n",
      "мен\n",
      "десят\n",
      "раз\n",
      "заключа\n",
      "и\n",
      "расторга\n",
      "союз\n",
      "с\n",
      "основн\n",
      "участник\n",
      "войн\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('russian')  # экземпляр класса \n",
    "example = ['Пердикка', 'не', 'менее', 'десяти', 'раз', 'заключал', 'и', 'расторгал', 'союзы', 'с', 'основными', 'участниками', 'войны', '.']\n",
    "for token in example:\n",
    "    print(stemmer.stem(token))  # stem() - метод класса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdbbe54-1183-4797-95c0-db3411c2386a",
   "metadata": {},
   "source": [
    "Другой, более крутой способ унифицировать словоформы - все-таки приводить их к словарной форме. В NLTK есть WordNetLemmatizer, который ищет нужные леммы в словаре. Работает только для английского. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "097a0d51-5b4a-43a3-b31f-8f15a73d5c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increase\n",
      "play\n",
      "play\n",
      "playing\n",
      "playing\n",
      "playing\n",
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('increases'))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"v\"))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"v\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"n\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"a\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"r\"))\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80862b49-5fad-4ae7-a01f-991c8f96ca7e",
   "metadata": {},
   "source": [
    "При обработке текстов для решения задач NLP, особенно если используются классические алгоритмы машинного обучения (современные нейронки извлекают признаки из всего, им часто лучше, чтобы текст сохранялся в исходном виде), бывает нужно выкинуть слишком распространенные и малозначимые слова: союзы, предлоги и т.п. В NLTK есть списки таких слов для разных языков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1210ad18-17d3-42dd-9632-4f2c3f595899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стоп-слова: {'зачем', 'бы', 'сам', 'другой', 'есть', 'была', 'много', 'под', 'чуть', 'да', 'при', 'между', 'всегда', 'же', 'сейчас', 'ним', 'тем', 'а', 'по', 'ну', 'эти', 'с', 'было', 'от', 'ей', 'будто', 'один', 'тебя', 'этом', 'какая', 'и', 'через', 'чтоб', 'три', 'вдруг', 'к', 'тот', 'совсем', 'был', 'разве', 'него', 'для', 'хорошо', 'они', 'вас', 'том', 'потому', 'над', 'нельзя', 'быть', 'этот', 'со', 'себе', 'опять', 'всего', 'мне', 'какой', 'ее', 'ничего', 'перед', 'где', 'на', 'когда', 'более', 'меня', 'без', 'о', 'кто', 'мы', 'нас', 'может', 'куда', 'ему', 'эту', 'моя', 'что', 'за', 'лучше', 'чтобы', 'можно', 'им', 'ж', 'во', 'его', 'того', 'этой', 'конечно', 'такой', 'чего', 'них', 'наконец', 'в', 'уж', 'здесь', 'иногда', 'ней', 'вы', 'два', 'еще', 'там', 'после', 'нибудь', 'не', 'из', 'нет', 'ли', 'если', 'никогда', 'этого', 'больше', 'были', 'теперь', 'про', 'до', 'почти', 'раз', 'я', 'так', 'но', 'ни', 'тогда', 'или', 'себя', 'все', 'мой', 'она', 'ведь', 'вот', 'только', 'хоть', 'чем', 'ты', 'об', 'нее', 'свою', 'их', 'он', 'вам', 'тут', 'надо', 'потом', 'у', 'как', 'будет', 'то', 'даже', 'всех', 'всю', 'уже', 'впрочем', 'тоже'}\n",
      "['обработке', 'текстов', 'решения', 'задач', 'NLP', ',', 'особенно', 'используются', 'классические', 'алгоритмы', 'машинного', 'обучения', '(', 'современные', 'нейронки', 'извлекают', 'признаки', ',', 'часто', ',', 'текст', 'сохранялся', 'исходном', 'виде', ')', ',', 'бывает', 'нужно', 'выкинуть', 'слишком', 'распространенные', 'малозначимые', 'слова', ':', 'союзы', ',', 'предлоги', 'т.п', '.', 'NLTK', 'списки', 'таких', 'слов', 'разных', 'языков']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = 'При обработке текстов для решения задач NLP, особенно если используются классические алгоритмы машинного обучения (современные нейронки извлекают признаки из всего, им часто лучше, чтобы текст сохранялся в исходном виде), бывает нужно выкинуть слишком распространенные и малозначимые слова: союзы, предлоги и т.п. В NLTK есть списки таких слов для разных языков'\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "print('Стоп-слова:', stop_words)\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f138bb09-da9b-465d-a2d7-f1919f1b18ae",
   "metadata": {},
   "source": [
    "Наконец к морфологии. Подробнее о ней мы поговорим в следующие разы, потому что морфологический разбор (с частями речи и еще и грамматическими характеристиками) - еще более сложная задача. В NLTK реализованы простейшие статистические парсеры, которые можно самостоятельно обучать на данных размеченных корпусов NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1be4395-4a81-4589-9558-f904e80c7df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = word_tokenize(\"And now for something completely different\")\n",
    "print(nltk.pos_tag(text))\n",
    "nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20051789-b132-4b97-81aa-f5b52b90ec4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PPSS'),\n",
       " ('want', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('analyze', None),\n",
       " ('sentences', 'NNS'),\n",
       " ('with', 'IN'),\n",
       " ('NLTK', None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown # Брауновский корпус\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news') # возьмем все размеченные предложения из новостей\n",
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents) # натренируем таггер на размеченных предложениях (он просто посчитает статистику)\n",
    "unigram_tagger.tag(word_tokenize('I want to analyze sentences with NLTK')) # попробуем на предложении, которого он не видел"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "660eae30-57d2-400d-95a4-b4d751a70b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PPSS'),\n",
       " ('want', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('analyze', None),\n",
       " ('sentences', None),\n",
       " ('with', None),\n",
       " ('NLTK', None)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(brown_tagged_sents)\n",
    "bigram_tagger.tag(word_tokenize('I want to analyze sentences with NLTK'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d01121-c301-4dcb-be92-8c22f0c1ef36",
   "metadata": {},
   "source": [
    "Bigram tagger работает как будто бы хуже, чем unigram, но в теории он может лучше справляться с омонимией (потому что учитывает рядом стоящее слово, и тогда beautiful book vs book something не разберет одинаково). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1fd52-c637-4d30-8525-8d5526fd91ee",
   "metadata": {},
   "source": [
    "Также NLTK умеет работать с данными базы [WordNet](https://wordnet.princeton.edu/). Оттуда он автоматически может извлекать сведения о семантических отношениях между словами, а также на данных WordNet у него реализован алгоритм Леска для решения задачи Word Sense Disambiguation. \n",
    "\n",
    "Это очень известная задача в NLP-мире, можно про нее подробнее посмотреть на [nlpprogress](http://nlpprogress.com/english/word_sense_disambiguation.html). Для ее решения мы должны неоднозначным словам в контекстах сопоставить дефиниции из словаря (в роли какового для английского языка успешно выступает WordNet). Алгоритм Леска был придуман в 1986 году и считается классическим подходом (бейзлайн, ага) для решения этой задачи. Мы предполагаем, что слова в заданном окне контекста (среди окружающих их слов) будут иметь похожую тематику. Это еще называется **дистрибутивная гипотеза** (\"Лингвистические единицы, встречающиеся в схожих контекстах, имеют близкие значения.\", придумали это лингвисты уже много лет назад). По алгоритму Леска, определение в словаре для целевого слова сравнивается со словами, которые стоят вокруг него в контексте. \n",
    "\n",
    "В базовой имплементации алгоритм Леска делает следующее:\n",
    "\n",
    "- считает количество слов, стоящих рядом с искомым словом и оказавшихся в словарном определении слова (для каждого варианта определения)\n",
    "- Каких слов больше всего оказалось, то и значение. \n",
    "\n",
    "Очень просто!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9c16998-783c-4c60-a885-de6286261978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "def get_semantic(seq, key_word):\n",
    "    temp = word_tokenize(seq)\n",
    "    temp = lesk(temp, key_word)\n",
    "    return temp.definition() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f1ff929-22d2-4874-be5c-090e04598928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrange for and reserve (something for someone else) in advance\n",
      "a number of sheets (ticket or stamps etc.) bound together on one edge\n"
     ]
    }
   ],
   "source": [
    "print(get_semantic('The table was already booked by someone else', 'book'))\n",
    "print(get_semantic('I love reading books on programming', 'book'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb5ae61-8d33-47e9-8db5-76bb5f23497d",
   "metadata": {},
   "source": [
    "Ну и синонимы с антонимами просто собираются напрямую из WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c82b9147-bd1c-45cf-8508-f48ca3e78e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synonyms ['dog.n.01', 'frump.n.01', 'dog.n.03', 'cad.n.01', 'frank.n.02', 'pawl.n.01', 'andiron.n.01', 'chase.v.01']\n",
      "['evil', 'evilness', 'bad', 'badness', 'bad', 'evil', 'ill']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "for syns in wordnet.synsets('dog'):\n",
    " synonyms.append(syns.name())\n",
    "print (\"synonyms\", synonyms)\n",
    "\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    " for l in syn.lemmas():\n",
    "  if l.antonyms():\n",
    "   antonyms.append(l.antonyms()[0].name())\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bcbe28-34c1-4a66-90b2-63b02ad51d25",
   "metadata": {},
   "source": [
    "#### SpaCy, corpy, UDPipe\n",
    "\n",
    "```pip install spacy```\n",
    "\n",
    "Спейси - современная библиотека, которая написана в Cython и использует нейронные сети. Интерфейс спейси довольно удобный и однообразный. Центральное понятие для спейси - это pipeline: то есть, набор действий, которые спейси совершает с текстом. \n",
    "\n",
    "Чтобы обработать текст на любом из языков, представленных в библиотеке ([список](https://spacy.io/usage/models)), достаточно завести пустой пайплайн:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139e8bc0-6e99-4636-b4fb-5c9d2de39836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "doc = nlp('My beautiful sentence is here.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3adfa4-3398-4be6-84fa-23a0403dfb27",
   "metadata": {},
   "source": [
    "Предупреждения может выдавать библиотека tensorflow, которая сообщает, что у вас не настроена CUDA, но их можно игнорировать, как и предлагается. \n",
    "\n",
    "Пустой пайплайн превращает наш текст в особый объект, в котором текст разделен на токены, и можно у этих токенов смотреть самые простые характеристики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c04d17-8c7e-4add-9f15-b8149a4f240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "№ 0. Токен:              My. Является словом: True. Является пунктуацией: False. Похож на число: False\n",
      "№ 1. Токен:       beautiful. Является словом: True. Является пунктуацией: False. Похож на число: False\n",
      "№ 2. Токен:        sentence. Является словом: True. Является пунктуацией: False. Похож на число: False\n",
      "№ 3. Токен:              is. Является словом: True. Является пунктуацией: False. Похож на число: False\n",
      "№ 4. Токен:            here. Является словом: True. Является пунктуацией: False. Похож на число: False\n",
      "№ 5. Токен:               .. Является словом: False. Является пунктуацией: True. Похож на число: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'№ {token.i}. Токен: {token.text:>15}. Является словом: {token.is_alpha}. Является пунктуацией: {token.is_punct}. Похож на число: {token.like_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59b277a-2430-4ea5-bfd9-03b896d30ab3",
   "metadata": {},
   "source": [
    "Объект doc также позволяет смотреть спаны (несколько токенов, срез текста):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14286bb-25ed-45d8-9432-26bf2673c67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beautiful sentence'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[1:3]\n",
    "span.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a81fe18-2acb-45c9-9f6a-9cb8efc2720a",
   "metadata": {},
   "source": [
    "Ну, это все прекрасно, но хотелось бы чего-то большего. Для spacy есть довольно много предобученных моделей для разных языков (список см. выше). Модельки нужно устанавливать (скачивать) с помощью команды в командной строке - она написана у них на сайте (python -m spacy download имя_модели). Когда вы загрузили модельку, вы можете ее использовать в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da2fe664-de61-4b10-b9a1-3c61f04b016a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57837ff8-0b5a-468c-9c83-84a9244c2b64",
   "metadata": {},
   "source": [
    "Теперь уже можно получить сведения поинтереснее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adb4ff73-b229-448f-980e-4eafa85273cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('The 4th Army was a Royal Yugoslav Army formation mobilised prior to the German-led invasion of the Kingdom of Yugoslavia during World War II.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e2234dc-2041-441b-8d68-3b0c0442f557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0. The             POS: DET    SyntR: det       Head: Army\n",
      " 1. 4th             POS: ADJ    SyntR: amod      Head: Army\n",
      " 2. Army            POS: PROPN  SyntR: nsubj     Head: was\n",
      " 3. was             POS: AUX    SyntR: ROOT      Head: was\n",
      " 4. a               POS: DET    SyntR: det       Head: formation\n",
      " 5. Royal           POS: PROPN  SyntR: compound  Head: Army\n",
      " 6. Yugoslav        POS: ADJ    SyntR: amod      Head: Army\n",
      " 7. Army            POS: PROPN  SyntR: compound  Head: formation\n",
      " 8. formation       POS: NOUN   SyntR: attr      Head: was\n",
      " 9. mobilised       POS: VERB   SyntR: acl       Head: formation\n",
      "10. prior           POS: ADV    SyntR: advmod    Head: mobilised\n",
      "11. to              POS: ADP    SyntR: prep      Head: prior\n",
      "12. the             POS: DET    SyntR: det       Head: invasion\n",
      "13. German          POS: PROPN  SyntR: npadvmod  Head: led\n",
      "14. -               POS: PUNCT  SyntR: punct     Head: led\n",
      "15. led             POS: VERB   SyntR: amod      Head: invasion\n",
      "16. invasion        POS: NOUN   SyntR: pobj      Head: to\n",
      "17. of              POS: ADP    SyntR: prep      Head: invasion\n",
      "18. the             POS: DET    SyntR: det       Head: Kingdom\n",
      "19. Kingdom         POS: PROPN  SyntR: pobj      Head: of\n",
      "20. of              POS: ADP    SyntR: prep      Head: Kingdom\n",
      "21. Yugoslavia      POS: PROPN  SyntR: pobj      Head: of\n",
      "22. during          POS: ADP    SyntR: prep      Head: mobilised\n",
      "23. World           POS: PROPN  SyntR: compound  Head: II\n",
      "24. War             POS: PROPN  SyntR: compound  Head: II\n",
      "25. II              POS: PROPN  SyntR: pobj      Head: during\n",
      "26. .               POS: PUNCT  SyntR: punct     Head: was\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'{token.i:2}. {token.text:15} POS: {token.pos_:6} SyntR: {token.dep_:9} Head: {token.head.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a33f7a-f241-4e40-85cc-ad9dad25a7c4",
   "metadata": {},
   "source": [
    "Грамматические характеристики можно тоже посмотреть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b98625d6-9b84-49d6-9abe-88289afa4b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definite=Def|PronType=Art\n",
      "Degree=Pos\n",
      "Number=Sing\n"
     ]
    }
   ],
   "source": [
    "for token in doc[:3]:\n",
    "    print(token.morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd07fce4-2fe0-4fd7-8db5-fcac7c565fd4",
   "metadata": {},
   "source": [
    "Также spacy позволяет разметить именованные сущности и посмотреть, что получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8491323c-524b-40b1-ac20-c7e0972102f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: The 4th Army                   Label: ORG\n",
      "Entity: Royal Yugoslav Army            Label: ORG\n",
      "Entity: German                         Label: NORP\n",
      "Entity: the Kingdom of Yugoslavia      Label: GPE\n",
      "Entity: World War II                   Label: EVENT\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'Entity: {ent.text:30} Label: {ent.label_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6141e6a-3403-4789-99d4-e8b4ef61e95f",
   "metadata": {},
   "source": [
    "Если аббревиатуры вас смущают, spacy легко предоставит расшифровки (для всех!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d96ca2fc-7571-427d-8aa5-11033c08ecdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nationalities or religious or political groups'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('NORP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0b9a387-3c26-42b0-b6d9-3b9dc579be6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'object of preposition'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('pobj')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b394478f-788c-4d02-9609-7ae5234b245d",
   "metadata": {},
   "source": [
    "Итак, а теперь самое интересное - это обертки для spacy + udpipe. UD, как известно - очень известный и влиятельный проект (чешский), цель которого - унифицировать тагсет для всех языков мира, причем как в морфологии, так и в синтаксисе. Неудивительно, что, поскольку в UD существует большое количество размеченных датасетов (русским языком активно занималась О. Ляшевская), то они обучили и свой парсер. Сам парсер написан в плюсах и не очень дружелюбен, но обертки для spacy делают жизнь проще. \n",
    "\n",
    "Внимание: я обычно это забываю, но прежде чем установить udpipe, **нужно поставить C++ Build Tools** (с сайта майкрософт, это бесплатно). Это нужно, чтобы скомпилировать udpipe из исходников в сях. Адрес сайта, откуда брать их, сам pip обычно подсказывает, но в целом можно просто погуглить. \n",
    "\n",
    "Две оболочки для udpipe, которые мы посмотрели - это spacy_udpipe и corpy. Обе нужно устанавливать:\n",
    "\n",
    "    pip install spacy_udpipe\n",
    "    pip install corpy\n",
    "    \n",
    "Для spacy_udpipe вообще ничего больше не нужно, там максимально автоматизированно скачиваются модельки. Для corpy приходится скачивать искомую модель руками, [отсюда](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3131). Эту модель вы можете положить куда угодно, главное потом указать corpy путь к ней. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b74c830-a2b5-4930-8693-09c931b46a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded a model for the 'en' language\n"
     ]
    }
   ],
   "source": [
    "import spacy_udpipe\n",
    "\n",
    "spacy_udpipe.download('en')  # эту команду достаточно выполнить только один раз - она как nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "445156c9-ed00-45c4-b027-13960a195c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0. The             Lemma: the             POS: DET    SyntR: det       Head: Army\n",
      " 1. 4th             Lemma: 4th             POS: ADJ    SyntR: amod      Head: Army\n",
      " 2. Army            Lemma: Army            POS: PROPN  SyntR: nsubj     Head: formation\n",
      " 3. was             Lemma: be              POS: AUX    SyntR: cop       Head: formation\n",
      " 4. a               Lemma: a               POS: DET    SyntR: det       Head: formation\n",
      " 5. Royal           Lemma: Royal           POS: PROPN  SyntR: compound  Head: Army\n",
      " 6. Yugoslav        Lemma: Yugoslav        POS: PROPN  SyntR: compound  Head: Army\n",
      " 7. Army            Lemma: Army            POS: PROPN  SyntR: compound  Head: formation\n",
      " 8. formation       Lemma: formation       POS: NOUN   SyntR: nsubj     Head: mobilised\n",
      " 9. mobilised       Lemma: mobilise        POS: VERB   SyntR: ROOT      Head: mobilised\n",
      "10. prior           Lemma: prior           POS: ADJ    SyntR: case      Head: invasion\n",
      "11. to              Lemma: to              POS: ADP    SyntR: fixed     Head: prior\n",
      "12. the             Lemma: the             POS: DET    SyntR: det       Head: invasion\n",
      "13. German          Lemma: german          POS: ADJ    SyntR: obl:npmod Head: led\n",
      "14. -               Lemma: -               POS: PUNCT  SyntR: punct     Head: led\n",
      "15. led             Lemma: lead            POS: VERB   SyntR: amod      Head: invasion\n",
      "16. invasion        Lemma: invasion        POS: NOUN   SyntR: obl       Head: mobilised\n",
      "17. of              Lemma: of              POS: ADP    SyntR: case      Head: Kingdom\n",
      "18. the             Lemma: the             POS: DET    SyntR: det       Head: Kingdom\n",
      "19. Kingdom         Lemma: Kingdom         POS: PROPN  SyntR: nmod      Head: invasion\n",
      "20. of              Lemma: of              POS: ADP    SyntR: case      Head: Yugoslavia\n",
      "21. Yugoslavia      Lemma: Yugoslavia      POS: PROPN  SyntR: nmod      Head: Kingdom\n",
      "22. during          Lemma: during          POS: ADP    SyntR: case      Head: II\n",
      "23. World           Lemma: World           POS: PROPN  SyntR: compound  Head: War\n",
      "24. War             Lemma: War             POS: PROPN  SyntR: compound  Head: II\n",
      "25. II              Lemma: ii              POS: PROPN  SyntR: nmod      Head: Kingdom\n",
      "26. .               Lemma: .               POS: PUNCT  SyntR: punct     Head: mobilised\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy_udpipe.load('en')\n",
    "doc = nlp('The 4th Army was a Royal Yugoslav Army formation mobilised prior to the German-led invasion of the Kingdom of Yugoslavia during World War II.')\n",
    "for token in doc:\n",
    "    print(f'{token.i:2}. {token.text:15} Lemma: {token.lemma_:15} POS: {token.pos_:6} SyntR: {token.dep_:9} Head: {token.head.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa2701-cdc4-4e6a-9a9e-5afe5b35e252",
   "metadata": {},
   "source": [
    "То же самое можно сделать в corpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "317031da-7235-4f99-87f9-5ac1e69e3a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpy.udpipe import Model\n",
    "\n",
    "model = Model('english-partut-ud-2.5-191206.udpipe')  \n",
    "# тут и нужно указать путь к вашей модели. Если она лежит в той же папке, что и скрипт, достаточно только имени файла\n",
    "\n",
    "sents = model.process('The 4th Army was a Royal Yugoslav Army formation mobilised prior to the German-led invasion of the Kingdom of Yugoslavia during World War II.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42116339-0961-4216-a1dc-d2b0721a708c",
   "metadata": {},
   "source": [
    "corpy возвращает генератор (то есть, итерируемый объект, который как магазин автомата, расстреляли все патроны - он опустел; повторно по генератору итерироваться нельзя). Генератор на каждом шаге выдает предложение (объект специального класса Sentence()), а в предложении - объекты класса Word(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a3c22ce6-1e8f-4ea6-a3bd-e3d9890ebede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<root>          Лемма: <root>          POS: <root> Грам. инфа: <root>\n",
      "The             Лемма: the             POS: DET Грам. инфа: Definite=Def|PronType=Art\n",
      "4th             Лемма: 4th             POS: ADJ Грам. инфа: Degree=Pos\n",
      "Army            Лемма: army            POS: NOUN Грам. инфа: Number=Sing\n",
      "was             Лемма: be              POS: AUX Грам. инфа: Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
      "a               Лемма: a               POS: DET Грам. инфа: Definite=Ind|Number=Sing|PronType=Art\n",
      "Royal           Лемма: royal           POS: PROPN Грам. инфа: \n",
      "Yugoslav        Лемма: Yugoslav        POS: PROPN Грам. инфа: \n",
      "Army            Лемма: army            POS: PROPN Грам. инфа: \n",
      "formation       Лемма: formation       POS: NOUN Грам. инфа: Number=Sing\n",
      "mobilised       Лемма: mobilize        POS: VERB Грам. инфа: Mood=Ind|Person=3|Tense=Past|VerbForm=Fin\n",
      "prior           Лемма: prior           POS: ADJ Грам. инфа: Degree=Pos\n",
      "to              Лемма: to              POS: ADP Грам. инфа: \n",
      "the             Лемма: the             POS: DET Грам. инфа: Definite=Def|PronType=Art\n",
      "German          Лемма: German          POS: PROPN Грам. инфа: \n",
      "-               Лемма: -               POS: PUNCT Грам. инфа: \n",
      "led             Лемма: lead            POS: VERB Грам. инфа: Tense=Past|VerbForm=Part\n",
      "invasion        Лемма: invasion        POS: NOUN Грам. инфа: Number=Sing\n",
      "of              Лемма: of              POS: ADP Грам. инфа: \n",
      "the             Лемма: the             POS: DET Грам. инфа: Definite=Def|PronType=Art\n",
      "Kingdom         Лемма: kingdom         POS: NOUN Грам. инфа: Number=Sing\n",
      "of              Лемма: of              POS: ADP Грам. инфа: \n",
      "Yugoslavia      Лемма: Yugoslavia      POS: PROPN Грам. инфа: \n",
      "during          Лемма: during          POS: ADP Грам. инфа: \n",
      "World           Лемма: world           POS: NOUN Грам. инфа: Number=Sing\n",
      "War             Лемма: war             POS: NOUN Грам. инфа: Number=Sing\n",
      "II              Лемма: second          POS: ADJ Грам. инфа: Degree=Pos|NumType=Ord\n",
      ".               Лемма: .               POS: PUNCT Грам. инфа: \n",
      "Алилуя!\n"
     ]
    }
   ],
   "source": [
    "for sent in sents:\n",
    "    for word in sent.words:\n",
    "        print(f'{word.form:15} Лемма: {word.lemma:15} POS: {word.upostag} Грам. инфа: {word.feats}')\n",
    "print('Алилуя!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f1c7fc-6865-4fb0-a527-23678ed323cd",
   "metadata": {},
   "source": [
    "у corpy, кстати, есть свой способ вывода имеющейся информации (хотя, по-моему, в юпитере и так красиво выводится...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa9de139-f229-4397-a6b6-5de7c8ec2c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\n",
      "   comments=[\n",
      "     '# newdoc',\n",
      "     '# newpar',\n",
      "     '# sent_id = 1',\n",
      "     '# text = The 4th Army was a Royal Yugoslav Army formation mobilised prior to the German-led invasion of the Kingdom of Yugoslavia during World War II.'],\n",
      "   words=[\n",
      "     Word(id=0, <root>),\n",
      "     Word(id=1,\n",
      "          form='The',\n",
      "          lemma='the',\n",
      "          xpostag='RD',\n",
      "          upostag='DET',\n",
      "          feats='Definite=Def|PronType=Art',\n",
      "          head=3,\n",
      "          deprel='det'),\n",
      "     Word(id=2,\n",
      "          form='4th',\n",
      "          lemma='4th',\n",
      "          xpostag='A',\n",
      "          upostag='ADJ',\n",
      "          feats='Degree=Pos',\n",
      "          head=3,\n",
      "          deprel='amod'),\n",
      "     Word(id=3,\n",
      "          form='Army',\n",
      "          lemma='army',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=9,\n",
      "          deprel='nsubj'),\n",
      "     Word(id=4,\n",
      "          form='was',\n",
      "          lemma='be',\n",
      "          xpostag='V',\n",
      "          upostag='AUX',\n",
      "          feats='Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin',\n",
      "          head=9,\n",
      "          deprel='cop'),\n",
      "     Word(id=5,\n",
      "          form='a',\n",
      "          lemma='a',\n",
      "          xpostag='RI',\n",
      "          upostag='DET',\n",
      "          feats='Definite=Ind|Number=Sing|PronType=Art',\n",
      "          head=9,\n",
      "          deprel='det'),\n",
      "     Word(id=6,\n",
      "          form='Royal',\n",
      "          lemma='royal',\n",
      "          xpostag='SP',\n",
      "          upostag='PROPN',\n",
      "          head=9,\n",
      "          deprel='nmod'),\n",
      "     Word(id=7,\n",
      "          form='Yugoslav',\n",
      "          lemma='Yugoslav',\n",
      "          xpostag='SP',\n",
      "          upostag='PROPN',\n",
      "          head=6,\n",
      "          deprel='flat'),\n",
      "     Word(id=8,\n",
      "          form='Army',\n",
      "          lemma='army',\n",
      "          xpostag='SP',\n",
      "          upostag='PROPN',\n",
      "          head=6,\n",
      "          deprel='flat'),\n",
      "     Word(id=9,\n",
      "          form='formation',\n",
      "          lemma='formation',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=10,\n",
      "          deprel='nsubj'),\n",
      "     Word(id=10,\n",
      "          form='mobilised',\n",
      "          lemma='mobilize',\n",
      "          xpostag='V',\n",
      "          upostag='VERB',\n",
      "          feats='Mood=Ind|Person=3|Tense=Past|VerbForm=Fin',\n",
      "          head=0,\n",
      "          deprel='root'),\n",
      "     Word(id=11,\n",
      "          form='prior',\n",
      "          lemma='prior',\n",
      "          xpostag='A',\n",
      "          upostag='ADJ',\n",
      "          feats='Degree=Pos',\n",
      "          head=10,\n",
      "          deprel='amod'),\n",
      "     Word(id=12,\n",
      "          form='to',\n",
      "          lemma='to',\n",
      "          xpostag='E',\n",
      "          upostag='ADP',\n",
      "          head=17,\n",
      "          deprel='case'),\n",
      "     Word(id=13,\n",
      "          form='the',\n",
      "          lemma='the',\n",
      "          xpostag='RD',\n",
      "          upostag='DET',\n",
      "          feats='Definite=Def|PronType=Art',\n",
      "          head=17,\n",
      "          deprel='det'),\n",
      "     Word(id=14,\n",
      "          form='German',\n",
      "          lemma='German',\n",
      "          xpostag='SP',\n",
      "          upostag='PROPN',\n",
      "          head=16,\n",
      "          deprel='obl',\n",
      "          misc='SpaceAfter=No'),\n",
      "     Word(id=15,\n",
      "          form='-',\n",
      "          lemma='-',\n",
      "          xpostag='FF',\n",
      "          upostag='PUNCT',\n",
      "          head=16,\n",
      "          deprel='punct',\n",
      "          misc='SpaceAfter=No'),\n",
      "     Word(id=16,\n",
      "          form='led',\n",
      "          lemma='lead',\n",
      "          xpostag='V',\n",
      "          upostag='VERB',\n",
      "          feats='Tense=Past|VerbForm=Part',\n",
      "          head=17,\n",
      "          deprel='acl'),\n",
      "     Word(id=17,\n",
      "          form='invasion',\n",
      "          lemma='invasion',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=10,\n",
      "          deprel='obl'),\n",
      "     Word(id=18,\n",
      "          form='of',\n",
      "          lemma='of',\n",
      "          xpostag='E',\n",
      "          upostag='ADP',\n",
      "          head=20,\n",
      "          deprel='case'),\n",
      "     Word(id=19,\n",
      "          form='the',\n",
      "          lemma='the',\n",
      "          xpostag='RD',\n",
      "          upostag='DET',\n",
      "          feats='Definite=Def|PronType=Art',\n",
      "          head=20,\n",
      "          deprel='det'),\n",
      "     Word(id=20,\n",
      "          form='Kingdom',\n",
      "          lemma='kingdom',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=17,\n",
      "          deprel='nmod'),\n",
      "     Word(id=21,\n",
      "          form='of',\n",
      "          lemma='of',\n",
      "          xpostag='E',\n",
      "          upostag='ADP',\n",
      "          head=22,\n",
      "          deprel='case'),\n",
      "     Word(id=22,\n",
      "          form='Yugoslavia',\n",
      "          lemma='Yugoslavia',\n",
      "          xpostag='SP',\n",
      "          upostag='PROPN',\n",
      "          head=20,\n",
      "          deprel='nmod'),\n",
      "     Word(id=23,\n",
      "          form='during',\n",
      "          lemma='during',\n",
      "          xpostag='E',\n",
      "          upostag='ADP',\n",
      "          head=25,\n",
      "          deprel='case'),\n",
      "     Word(id=24,\n",
      "          form='World',\n",
      "          lemma='world',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=25,\n",
      "          deprel='nmod'),\n",
      "     Word(id=25,\n",
      "          form='War',\n",
      "          lemma='war',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=10,\n",
      "          deprel='obl'),\n",
      "     Word(id=26,\n",
      "          form='II',\n",
      "          lemma='second',\n",
      "          xpostag='NO',\n",
      "          upostag='ADJ',\n",
      "          feats='Degree=Pos|NumType=Ord',\n",
      "          head=25,\n",
      "          deprel='amod',\n",
      "          misc='SpaceAfter=No'),\n",
      "     Word(id=27,\n",
      "          form='.',\n",
      "          lemma='.',\n",
      "          xpostag='FS',\n",
      "          upostag='PUNCT',\n",
      "          head=10,\n",
      "          deprel='punct',\n",
      "          misc='SpaceAfter=No')])]\n"
     ]
    }
   ],
   "source": [
    "from corpy.udpipe import pprint\n",
    "\n",
    "pprint(list(model.process('The 4th Army was a Royal Yugoslav Army formation mobilised prior to the German-led invasion of the Kingdom of Yugoslavia during World War II.')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b79da0-fdc9-4cdb-b4da-52a0e4e647a7",
   "metadata": {
    "id": "5GU9unbnvJXP"
   },
   "source": [
    "#### Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5160823-1fa8-4679-ae46-92d29ba761ec",
   "metadata": {
    "id": "eKwCF6UOvRCh"
   },
   "outputs": [],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546f15a2-a754-4677-b97b-a46a7a39efb5",
   "metadata": {
    "id": "Zv7BrtxEvUwY"
   },
   "source": [
    "Загрузка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52b28cfd-5afc-4016-ab4b-a4a6092b8fe6",
   "metadata": {
    "executionInfo": {
     "elapsed": 2995,
     "status": "ok",
     "timestamp": 1663154625797,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "8UtSgSntvXro"
   },
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4532522-79d4-4016-812f-5b0bc8e6fe47",
   "metadata": {
    "id": "P7VKLnRkvZd4"
   },
   "outputs": [],
   "source": [
    "nlp_ru = stanza.Pipeline(lang='ru')\n",
    "nlp_en = stanza.Pipeline(lang='en', processors='tokenize, pos, constituency')\n",
    "nlp_fr = stanza.Pipeline(lang='fr', processors='tokenize, mwt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03976ce0-7225-4e3e-ba5b-ba132a5494e0",
   "metadata": {
    "id": "dUR7LXjmvtpn"
   },
   "source": [
    "Токенизация, сегментация по предложениям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fe41607-2b8f-469d-84ad-9f073d539462",
   "metadata": {
    "executionInfo": {
     "elapsed": 2242,
     "status": "ok",
     "timestamp": 1663154958031,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "Db8CYc_SvtIX"
   },
   "outputs": [],
   "source": [
    "text = '''Архитектура Византии — совокупность традиций строительства и архитектуры в поздней Римской империи и в Византии в период с начала IV века по середину XV века. В качестве отдельных направлений исследования выделяют религиозную архитектуру Византии, византийскую фортификацию и гражданское строительство, включающее дворцы, общественные сооружения и частные дома. Также в рамках данной дисциплины изучают традиции строительного ремесла и декоративного искусства.'''\n",
    "\n",
    "doc = nlp_ru(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "159262da-5400-45b8-b826-a5b29d0967de",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1663154817857,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "MMT0yrtGv3wP",
    "outputId": "2e4e3a15-9345-47c7-ed71-e4c8fc17dc2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Архитектура Византии — совокупность традиций строительства и архитектуры в поздней Римской империи и в Византии в период с начала IV века по середину XV века.\n",
      "В качестве отдельных направлений исследования выделяют религиозную архитектуру Византии, византийскую фортификацию и гражданское строительство, включающее дворцы, общественные сооружения и частные дома.\n",
      "Также в рамках данной дисциплины изучают традиции строительного ремесла и декоративного искусства.\n"
     ]
    }
   ],
   "source": [
    "print(*[sentence.text for sentence in doc.sentences], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d705227-8e8a-4d95-adbd-d2be862b46bb",
   "metadata": {
    "id": "IxBb4XOMwJdf"
   },
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(doc.sentences):\n",
    "  print(f'====== Sentence {i + 1} tokens =======')\n",
    "  print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939aa8c1-0cec-49b7-90e6-e75fedd39eff",
   "metadata": {
    "id": "i2cRJqFSwO-v"
   },
   "outputs": [],
   "source": [
    "text_fr = '''Il est révélé par les romans Extension du domaine de la lutte (1994) et, surtout, Les Particules élémentaires (1998), qui le fait connaître d'un large public.'''\n",
    "\n",
    "doc_fr = nlp_fr(text_fr)\n",
    "for token in doc_fr.sentences[0].tokens:\n",
    "    print(f'token: {token.text}\\twords: {\", \".join([word.text for word in token.words])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e28518f-8b4d-4a12-a1bf-b485d831affb",
   "metadata": {
    "id": "HlS7SKLUwetI"
   },
   "source": [
    "Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54ffec-e524-47f3-8c02-e0313f8c6109",
   "metadata": {
    "id": "Fh6HPJn2wgcp"
   },
   "outputs": [],
   "source": [
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c56f643-dd1e-4cc2-8895-3e851bd9ae14",
   "metadata": {
    "id": "5SsW-5MFwupQ"
   },
   "source": [
    "Морфопарсинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cad073-6147-4399-a096-eb3044d3727a",
   "metadata": {
    "id": "Z2wxiB8NwtYf"
   },
   "outputs": [],
   "source": [
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf8d0d-3cf0-4ed9-8f7a-3d76932c2d72",
   "metadata": {
    "id": "bKZwX5l8wy6Q"
   },
   "source": [
    "Парсинг синтаксических зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f9527c-37fd-4bd1-b57d-cf50eb65cabe",
   "metadata": {
    "id": "w11Jqo6Fwx14"
   },
   "outputs": [],
   "source": [
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85919338-eb05-4504-9e93-cb21e395410c",
   "metadata": {
    "id": "k7zTW3sJw27A"
   },
   "outputs": [],
   "source": [
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976357e-d041-42c1-952f-19a2316121cf",
   "metadata": {
    "id": "8-1cXBtWw5K4"
   },
   "source": [
    "Парсинг составляющих (для русского недоступен)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b41130-faff-47e9-b41b-4d860d260b5e",
   "metadata": {
    "id": "5KGb8sV6w6lo"
   },
   "outputs": [],
   "source": [
    "doc_en = nlp_en('This is a sentence for parsing constituencies.')\n",
    "\n",
    "for sentence in doc_en.sentences:\n",
    "    print(sentence.constituency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a313cfb6-7b59-4d2d-9a2f-5bf2135b536e",
   "metadata": {
    "id": "HWfmZei5t_8I"
   },
   "source": [
    "#### natasha\n",
    "\n",
    "Библиотека natasha - это библиотека только для русского языка. Она очень быстрая и модульная, так что можно устанавливать только какие-то отдельные вещи из нее, например, у нее есть отдельный подмодуль razdel с токенизатором и сентенайзером:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6113f76-a149-480d-8fa6-7196ef25448d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Раньше', 'библиотека', 'Natasha', 'решала', 'задачу', 'NER', 'для', 'русского', 'языка', ',', 'была', 'построена', 'на', 'правилах', ',', 'показывала', 'среднее', 'качество', 'и', 'производительность', '.']\n",
      "['Сейчас', 'Natasha', '—', 'это', 'целый', 'большой', 'проект', ',', 'состоит', 'из', '9', 'репозиториев', '.']\n",
      "['Библиотека', 'Natasha', 'объединяет', 'их', 'под', 'одним', 'интерфейсом', ',', 'решает', 'базовые', 'задачи', 'обработки', 'естественного', 'русского', 'языка', ':', 'сегментация', 'на', 'токены', 'и', 'предложения', ',', 'предобученные', 'эмбеддинги', ',', 'анализ', 'морфологии', 'и', 'синтаксиса', ',', 'лемматизация', ',', 'NER', '.']\n",
      "['Все', 'решения', 'показывают', 'топовые', 'результаты', 'в', 'новостной', 'тематике', ',', 'быстро', 'работают', 'на', 'CPU', '.']\n"
     ]
    }
   ],
   "source": [
    "!pip install razdel\n",
    "\n",
    "from razdel import sentenize, tokenize\n",
    "\n",
    "raw = '''Раньше библиотека Natasha решала задачу NER для русского языка, была построена на правилах, показывала среднее качество и производительность. Сейчас Natasha — это целый большой проект, состоит из 9 репозиториев. Библиотека Natasha объединяет их под одним интерфейсом, решает базовые задачи обработки естественного русского языка: сегментация на токены и предложения, предобученные эмбеддинги, анализ морфологии и синтаксиса, лемматизация, NER. Все решения показывают топовые результаты в новостной тематике, быстро работают на CPU.'''\n",
    "sents = [sent.text for sent in sentenize(raw)]\n",
    "for i in range(len(sents)):\n",
    "    sents[i] = [token.text for token in tokenize(sents[i])]\n",
    "for sent in sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a34e6-8202-47bb-8e91-ba89c8a672e2",
   "metadata": {
    "id": "Fw52LhAct9Tg"
   },
   "outputs": [],
   "source": [
    "!pip install natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bcd822-1878-4a62-87cb-152414f03d1d",
   "metadata": {
    "id": "8VqX_VxouI-q"
   },
   "source": [
    "Морфосинтаксический парсинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c012a2-a9be-4d1b-9e7d-a32a41ce50dd",
   "metadata": {
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1663154496060,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "S5k9Wlz9uK7M"
   },
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    \n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b4ccf2-8f7d-489f-9649-52bf52282d39",
   "metadata": {
    "id": "ciOkGldHuLgB"
   },
   "outputs": [],
   "source": [
    "segmenter = Segmenter()  # токенизация и разделение на предложения\n",
    "emb = NewsEmbedding()  # эмбеддинги\n",
    "morph_tagger = NewsMorphTagger(emb)  # морфология\n",
    "syntax_parser = NewsSyntaxParser(emb) # синтаксис\n",
    "\n",
    "text = '29 марта 2017 года правительство Великобритании инициировало процедуру выхода в соответствии со статьёй 50 Договора о Европейском союзе; первоначально планировалось, что Великобритания покинет Европейский союз через два года, 29 марта 2019 года в 23:00 по Гринвичу.'\n",
    "doc = Doc(text)\n",
    "\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "doc.parse_syntax(syntax_parser)\n",
    "sent = doc.sents[0]\n",
    "sent.morph.print()\n",
    "sent.syntax.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ebd18e-7438-49f8-80c3-cded10d76517",
   "metadata": {
    "id": "YPCVWdB1ujFp"
   },
   "source": [
    "Распознание именованных сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aecbf1c-bab1-4573-b7f7-2bc3344d43f3",
   "metadata": {
    "id": "AaTUTjyMuikI"
   },
   "outputs": [],
   "source": [
    "from natasha import NewsNERTagger\n",
    "\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "doc.tag_ner(ner_tagger)\n",
    "doc.ner.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb61420-d7e1-4f5d-9a1b-f48bbfcdb22f",
   "metadata": {
    "id": "Log6uopyux7n"
   },
   "source": [
    "Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd9ce66-f27e-4f92-8567-d1eacb7724f5",
   "metadata": {
    "id": "RNSGZViPuuJd"
   },
   "outputs": [],
   "source": [
    "from natasha import MorphVocab\n",
    "\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "for token in doc.tokens:\n",
    "  token.lemmatize(morph_vocab)\n",
    "  print(token.lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57692933-368c-4f86-8642-f93422311a5d",
   "metadata": {
    "id": "o1dqpS_Hu3OT"
   },
   "source": [
    "Нормализация именованных сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dcb6dc-a7ab-434e-86b3-98455b8cb2be",
   "metadata": {
    "id": "gqmxRFqbu2m8"
   },
   "outputs": [],
   "source": [
    "for span in doc.spans:\n",
    "    span.normalize(morph_vocab)\n",
    "   \n",
    "{_.text: _.normal for _ in doc.spans}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17b54f-893b-4fec-a9fc-22d598e94981",
   "metadata": {},
   "source": [
    "### Морфопарсеры для русского языка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06facb48-dc8a-408e-bb19-adc942086ccf",
   "metadata": {},
   "source": [
    "Некоторые инструменты существуют сами по себе, без большой общей библиотеки; для русского языка существует очень популярный правиловый морфопарсер pymorphy2, который можно использовать для быстрой (и не слишком качественной) лемматизации, например, когда важнее скорость, чем качество. Также есть морфопарсер rnnmorph на нейронной сети. \n",
    "\n",
    "Pymorphy устанавливается так:\n",
    "\n",
    "```\n",
    "pip install pymorphy2\n",
    "pip install -U pymorphy2-dicts-ru  # необязательно: для обновления словаря\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ea1852-4e5a-4fc4-a1bd-ab37378be366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='студентки', tag=OpencorporaTag('NOUN,anim,femn sing,gent'), normal_form='студентка', score=0.6, methods_stack=((DictionaryAnalyzer(), 'студентки', 40, 1),)),\n",
       " Parse(word='студентки', tag=OpencorporaTag('NOUN,anim,femn plur,nomn'), normal_form='студентка', score=0.4, methods_stack=((DictionaryAnalyzer(), 'студентки', 40, 7),))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "parse = morph.parse('студентки')\n",
    "parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615cab78-1361-4245-92c5-63602807efb4",
   "metadata": {},
   "source": [
    "У класса MorphAnalyzer() есть метод parse, который возвращает что? Список экземпляров класса Parse. У этого класса есть свои атрибуты: word (исходная форма слова), tag (грам. инфа), normal_form (лемма), score(предпосчитанная на OpenCorpora вероятность правильности разбора) и несколько технических. \n",
    "\n",
    "Соответственно, получить информацию можно, просто обращаясь к атрибутам (не забудьте, что у нас всегда список, поэтому нужно еще и индекс разбора указывать):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78d34911-cf78-4fe9-98f4-193476ebfd6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "студентки\n",
      "NOUN,anim,femn sing,gent\n",
      "студентка\n"
     ]
    }
   ],
   "source": [
    "print(parse[0].word)\n",
    "print(parse[0].tag)\n",
    "print(parse[0].normal_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1757682-67fc-48d1-8f8f-ae94856df350",
   "metadata": {},
   "source": [
    "Атрибут tag &ndash; это экземпляр класса OpencorporaTag, как можно догадаться. У него есть еще свои атрибуты, к которым тоже можно обращаться, чтобы получать более конкретную информацию о слове. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c64125a-6f50-4e82-9401-d9727e804572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Часть речи: NOUN\n",
      "Одушевленность: anim\n",
      "Падеж: nomn\n",
      "Род: masc\n",
      "Наклонение: None\n",
      "Число: sing\n",
      "Лицо: None\n",
      "Время: None\n",
      "Переходность: None\n",
      "Залог: None\n"
     ]
    }
   ],
   "source": [
    "parse = morph.parse('участник')\n",
    "\n",
    "t = parse[0].tag  # я записала в переменную, просто чтобы не копировать каждый раз все целиком\n",
    "# но это то же самое, что parse[0].tag.animacy...\n",
    "print(f'Часть речи: {t.POS}')\n",
    "print(f'Одушевленность: {t.animacy}\\nПадеж: {t.case}\\nРод: {t.gender}\\nНаклонение: {t.mood}\\\n",
    "\\nЧисло: {t.number}\\nЛицо: {t.person}\\nВремя: {t.tense}\\nПереходность: {t.transitivity}\\nЗалог: {t.voice}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce95e66f-3b76-48fe-9718-8e8ee82d8b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Часть речи: VERB\n",
      "Одушевленность: None\n",
      "Падеж: None\n",
      "Род: None\n",
      "Наклонение: indc\n",
      "Число: sing\n",
      "Лицо: 3per\n",
      "Время: pres\n",
      "Переходность: tran\n",
      "Залог: None\n"
     ]
    }
   ],
   "source": [
    "parse = morph.parse('говорит')\n",
    "\n",
    "t = parse[0].tag  \n",
    "print(f'Часть речи: {t.POS}')\n",
    "print(f'Одушевленность: {t.animacy}\\nПадеж: {t.case}\\nРод: {t.gender}\\n\\\n",
    "Наклонение: {t.mood}\\nЧисло: {t.number}\\nЛицо: {t.person}\\nВремя: {t.tense}\\nПереходность: {t.transitivity}\\nЗалог: {t.voice}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fbfe36-c880-4b67-86b6-4f62eb0498fd",
   "metadata": {},
   "source": [
    "Если вы запрашиваете категорию, которой у данного слова нет (ну нет переходности у существительного), вернется None. \n",
    "\n",
    "Также можно попросить pymorphy поставить слово в конкретную форму или вообще вернуть всю парадигму. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9c51870-5a49-4116-bbb4-b2952b7d6820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='говорят', tag=OpencorporaTag('VERB,impf,tran plur,3per,pres,indc'), normal_form='говорить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'говорят', 415, 6),))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].inflect({'plur'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c18baa16-240d-493d-bade-c67a123b5de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='участник', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участник', 2, 0),)),\n",
       " Parse(word='участника', tag=OpencorporaTag('NOUN,anim,masc sing,gent'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участника', 2, 1),)),\n",
       " Parse(word='участнику', tag=OpencorporaTag('NOUN,anim,masc sing,datv'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участнику', 2, 2),)),\n",
       " Parse(word='участника', tag=OpencorporaTag('NOUN,anim,masc sing,accs'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участника', 2, 3),)),\n",
       " Parse(word='участником', tag=OpencorporaTag('NOUN,anim,masc sing,ablt'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участником', 2, 4),)),\n",
       " Parse(word='участнике', tag=OpencorporaTag('NOUN,anim,masc sing,loct'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участнике', 2, 5),)),\n",
       " Parse(word='участники', tag=OpencorporaTag('NOUN,anim,masc plur,nomn'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участники', 2, 6),)),\n",
       " Parse(word='участников', tag=OpencorporaTag('NOUN,anim,masc plur,gent'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участников', 2, 7),)),\n",
       " Parse(word='участникам', tag=OpencorporaTag('NOUN,anim,masc plur,datv'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участникам', 2, 8),)),\n",
       " Parse(word='участников', tag=OpencorporaTag('NOUN,anim,masc plur,accs'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участников', 2, 9),)),\n",
       " Parse(word='участниками', tag=OpencorporaTag('NOUN,anim,masc plur,ablt'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участниками', 2, 10),)),\n",
       " Parse(word='участниках', tag=OpencorporaTag('NOUN,anim,masc plur,loct'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участниках', 2, 11),))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].lexeme  \n",
    "# парадигму глагола лучше не выводить - она длинная; я перед запуском этой ячейки перезапустила разбор с существительным, поэтому не удивляйтесь. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b209f076-7bf2-4130-99d7-dc960fd68f92",
   "metadata": {},
   "source": [
    "Наконец, можно попросить pymorphy выводить грам. информацию по-русски:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bee9dca8-09d7-4516-a590-7fa26990e8fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'СУЩ,од,мр ед,им'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].tag.cyr_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4854f61-d240-42fd-a560-3d732285ddb8",
   "metadata": {},
   "source": [
    "Pymorphy очень быстро работает и имеет много возможностей, но совершенно не умеет разрешать омонимию и никак не учитывает контекст."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978a8156-4c81-41c4-81b9-85c017922e2e",
   "metadata": {},
   "source": [
    "rnnmorph - инструмент покруче (в 2017 году его автор выиграл соревнование по морфопарсингу русского языка). \n",
    "\n",
    "Устанавливается просто:\n",
    "\n",
    "```pip install rnnmorph```\n",
    "\n",
    "Правда, может козлить во время установки. Иногда, если плохо установился и не работает, приходится его удалять командой pip uninstall rnnmorph (обязательно в консоли от имени администратора!). Периодически, если обновляется версия tensorflow, может перестать работать &ndash; но Илья Гусев обычно вскоре обновляет и свой парсер, так что достаточно следить за новостями в [его гитхабе](https://github.com/IlyaGusev/rnnmorph) &ndash; или просто подождать светлых времен."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15650e62-6eb4-494e-ad47-aa37c60e6733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 12:00:12.010260: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-06 12:00:12.010289: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-04-06 12:00:13.910063: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-06 12:00:13.910109: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-06 12:00:13.910134: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (aslin): /proc/driver/nvidia/version does not exist\n",
      "2022-04-06 12:00:13.910346: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<normal_form=пердикк; word=Пердикка; pos=NOUN; tag=Case=Acc|Gender=Masc|Number=Sing; score=0.7922>,\n",
       " <normal_form=не; word=не; pos=PART; tag=_; score=1.0000>,\n",
       " <normal_form=менее; word=менее; pos=ADV; tag=Degree=Cmp; score=1.0000>,\n",
       " <normal_form=десять; word=десяти; pos=NUM; tag=Case=Gen; score=1.0000>,\n",
       " <normal_form=раз; word=раз; pos=NOUN; tag=Case=Gen|Gender=Masc|Number=Plur; score=0.9998>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "\n",
    "predictor = RNNMorphPredictor(language='ru')\n",
    "parse = predictor.predict(['Пердикка', 'не', 'менее', 'десяти', 'раз', 'заключал', 'и', 'расторгал', 'союзы', 'с', 'основными', 'участниками', 'войны', '.'])\n",
    "parse[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b592bc04-3b8d-4a3d-b3c3-f81886f723ef",
   "metadata": {},
   "source": [
    "rnnmorph принимает список строк (токенов) и возвращает список объектов специального класса, у которого есть атрибуты normal_form (лемма), word (исходная форма), pos (часть речи), tag (грам. информация) и score (уверенность нейронной модели в правильности своего ответа). Он умеет снимать омонимию (лучше, чем майстем, но хуже, чем интегральный морфопарсер). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "007f5dc1-4580-4a7d-bf65-f8d5fb55ac12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[-2].pos  # например, можно узнать часть речи для предпоследнего слова в списке"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760e1f4d-463c-4fc2-bc9b-ec11b0d4b684",
   "metadata": {},
   "source": [
    "### Просмотр разметки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26e4c03-51f2-4547-94bb-bb8abf16973f",
   "metadata": {},
   "source": [
    "В дальнейшем нам с вами часто придется работать с форматом conllu, в котором хранится разметка UD, например. Для чтения файлов в этом формате существуют готовые библиотеки: conllu и pyconll. \n",
    "\n",
    "Например, как пользоваться последней:\n",
    "\n",
    "```pip install pyconll```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af344136-1d20-4009-81b0-7d26af3f8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll\n",
    "\n",
    "text = pyconll.load_from_file('myfile.conllu')\n",
    "\n",
    "for sentence in text:\n",
    "    for token in sentence:\n",
    "        print(token.id, token.form, token.lemma, token.upos, token.feats, token.head, token.deprel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce790f8-4e1c-4949-a54c-72728f1cb1e9",
   "metadata": {},
   "source": [
    "Естественно, можно не только печатать информацию, но и добавлять в список и вообще делать все, что угодно. Что это за атрибуты у токенов?\n",
    "\n",
    "- id - порядковый номер токена в предложении\n",
    "- form - исходная форма\n",
    "- lemma - лемма\n",
    "- upos - часть речи в UD\n",
    "- xpos - часть речи в неуниверсальном формате (обычно встречается, если датасет конвертированный)\n",
    "- feats - грам. характеристики\n",
    "- head - расстояние от синтаксической вершины\n",
    "- deprel - тип синтаксической связи\n",
    "- две зарезервированные ячейки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b36099-43a2-47a9-b4d6-10d8862c414b",
   "metadata": {},
   "source": [
    "Где можно красивенько отрисовать .conllu файлы в виде деревьев зависимости:\n",
    "\n",
    "[Арборатор](https://arborator.ilpga.fr/q.cgi): достаточно вставить текст в формате .conllu. Иногда отваливается, правда. \n",
    "\n",
    "[Conllu-Viewer на сайте UD](https://universaldependencies.org/conllu_viewer.html): умеет читать файлы и рисовать последовательно все предложения\n",
    "\n",
    "Для затравки вот картинка с арборатора:\n",
    "\n",
    "<img src='https://github.com/rsuh-python/bac20-24/blob/main/6%20term%20(spring%202023)/09-Morphology/arbo.png?raw=true'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00f691d-b228-40de-83f0-8a7b7155f13c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
