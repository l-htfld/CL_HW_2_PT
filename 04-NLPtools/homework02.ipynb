{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727bd75d-3dfc-4fe7-b4b6-a251628810a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Задача 1 (10 баллов). \n",
    "\n",
    "Попробуем себя в решении задачи определения темы текста. Будем считать, что два текста похожи по теме, если у них больше общих слов (только не предлогов с союзами), чем у других текстов. У нашей программы для определения темы будет несколько готовых текстов (достаточно больших!) с уже известной темой в базе: выберите тексты (и темы) самостоятельно, 5-6 будет достаточно. \n",
    "\n",
    "Что должна делать программа? При запуске вы ей сообщаете название нового файла с текстом, который нужно классифицировать, она его открывает, обрабатывает и сравнивает с текстами в своей базе. С которым из текстов оказалось больше всего общих слов, того и тема! Очевидно, вам понадобится какие-то слова из текстов отбрасывать (подумайте, каким образом это сделать - здесь на самом деле несколько вариантов концепций), а еще лемматизировать или хотя бы использовать стемминг. \n",
    "\n",
    "Когда будете сдавать это задание, пожалуйста, пришлите и файлы с текстами. И имейте в виду, если тексты будут вставлены прямо в код и слишком короткие, я задачу засчитаю только вполовину. \n",
    "\n",
    "Напоминаю, как открываются файлы:\n",
    "\n",
    "```\n",
    "with open('путь к файлу - пишите прямые слеши', 'r', encoding='utf-8') as f:\n",
    "    text = f.read() # все содержимое вашего файла считается в одну длинную строку\n",
    "```\n",
    "\n",
    "Настоятельно советую оформить код хотя бы в функции. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f44dfb9-10a8-401c-9e56-74a8ecb6a00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "with open(r\"C:\\Users\\HP\\Documents\\python\\IntroCL2023\\04-NLPtools\\тестовый текст.txt\", 'r', encoding='utf-8') as f:\n",
    "   text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower() # приводим текст в нижний регистр\n",
    "words = nltk.word_tokenize(text) # токенизируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('russian'))\n",
    "words = [word for word in words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['еда', 'играет', 'важную', 'роль', 'жизни', 'каждого', 'человека', '.', 'обеспечивает', 'наш', 'организм', 'необходимыми', 'питательными', 'веществами', 'энергией', 'поддержания', 'здоровья', 'жизненных', 'функций', '.', 'кроме', ',', 'еда', 'является', 'незаменимой', 'частью', 'культуры', 'традиций', 'различных', 'народов', '.', 'моей', 'жизни', 'еда', 'также', 'имеет', 'особое', 'значение', '.', 'считаю', ',', 'правильное', 'питание', '-', 'это', 'ключ', 'здоровому', 'образу', 'жизни', '.', 'поэтому', 'стараюсь', 'следить', 'своим', 'рационом', 'придерживаться', 'принципов', 'здорового', 'питания', '.', 'убежден', ',', 'правильный', 'выбор', 'продуктов', 'питания', 'позволяет', 'сохранять', 'хорошее', 'здоровье', 'энергию', 'весь', 'день', '.', 'однако', ',', 'считаю', 'фанатом', 'строгой', 'диеты', 'исключения', 'каких-либо', 'продуктов', 'своего', 'рациона', '.', 'верю', ',', 'должно', 'меру', ',', 'поэтому', 'позволяю', 'наслаждаться', 'любимыми', 'блюдами', 'время', 'времени', '.', 'например', ',', 'обожаю', 'пиццу', 'шоколад', ',', 'стараюсь', 'злоупотреблять', 'ими', '.', 'кроме', ',', 'еда', '-', 'это', 'возможность', 'познакомиться', 'различными', 'культурами', 'традициями', '.', 'люблю', 'пробовать', 'новые', 'блюда', 'экспериментировать', 'кулинарией', '.', 'готовлю', 'самостоятельно', 'кухне', ',', 'хожу', 'рестораны', 'открываю', 'новые', 'вкусы', '.', 'также', 'еда', '-', 'это', 'возможность', 'общения', 'друзьями', 'близкими', '.', 'часто', 'собираемся', 'столом', ',', 'пообщаться', ',', 'поделиться', 'новостями', 'насладиться', 'вкусной', 'едой', '.', 'считаю', ',', 'такие', 'моменты', 'очень', 'важны', 'нашего', 'эмоционального', 'здоровья', 'укрепления', 'отношений', 'близкими', 'людьми', '.', 'целом', ',', 'еда', 'играет', 'огромную', 'роль', 'моей', 'жизни', ',', 'физически', ',', 'эмоционально', '.', 'стараюсь', 'придерживаться', 'здорового', 'образа', 'жизни', ',', 'забываю', ',', 'еда', 'должна', 'приносить', 'удовольствие', 'радость', '.']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b0ecd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"C:\\Users\\HP\\Documents\\python\\IntroCL2023\\04-NLPtools\\Темы текстов\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_texts = []\n",
    "\n",
    "# перебираем все файлы в папке\n",
    "for folder_name in os.listdir(folder_path):\n",
    "  folder_full_path = os.path.join(folder_path, folder_name)\n",
    "  \n",
    "  # проверяем, является ли текущий элемент папкой\n",
    "  if os.path.isdir(folder_full_path):\n",
    "      # Перебираем все файлы в текущей папке\n",
    "      for file_name in os.listdir(folder_full_path):\n",
    "          file_full_path = os.path.join(folder_full_path, file_name)\n",
    "          \n",
    "          # проверяем, является ли текущий элемент файлом\n",
    "          if os.path.isfile(file_full_path):\n",
    "              # читаем содержимое файла\n",
    "              with open(file_full_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                # преобразуем текст в список слов\n",
    "                words = text.split()\n",
    "                base_texts.append(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тема текста: Еда\n"
     ]
    }
   ],
   "source": [
    "new_text_words = set(lemmatized_words)\n",
    "\n",
    "max_common_words = 0\n",
    "max_common_text = None\n",
    "\n",
    "for folder_name in os.listdir(folder_path):\n",
    " folder_full_path = os.path.join(folder_path, folder_name)\n",
    " \n",
    " # проверяем, является ли текущий элемент папкой\n",
    " if os.path.isdir(folder_full_path):\n",
    "     # Перебираем все файлы в текущей папке\n",
    "     for file_name in os.listdir(folder_full_path):\n",
    "         file_full_path = os.path.join(folder_full_path, file_name)\n",
    "         \n",
    "         # проверяем, является ли текущий элемент файлом\n",
    "         if os.path.isfile(file_full_path):\n",
    "             # читаем содержимое файла\n",
    "             with open(file_full_path, 'r', encoding='utf-8') as f:\n",
    "               text = f.read()\n",
    "               words = text.split()\n",
    "               common_words = set(words).intersection(new_text_words)\n",
    "               if len(common_words) > max_common_words:\n",
    "                  max_common_words = len(common_words)\n",
    "                  max_common_text = folder_name\n",
    "\n",
    "print(f\"Тема текста: {max_common_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f7c4b-0934-4f27-a7b0-edf84185bbf6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Задача 2 (10 баллов). \n",
    "\n",
    "Некоторые предлоги в русском языке могут управлять разными падежами (например, \"я еду в Лондон\" vs \"я живу в Лондоне\"). Давайте проанализируем эти предлоги и их падежи. Необходимо:\n",
    "\n",
    "- составить список таких предлогов (РГ-80 вам в помощь)\n",
    "- взять достаточно большой текст (можно большое художественное произведение)\n",
    "- сделать морфоразбор этого текста\n",
    "- Посчитать, как часто и какие падежи встречаются у слова, идущего после предлога.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115add3c",
   "metadata": {},
   "source": [
    "\n",
    "Примечания: во-первых, имейте в виду, что иногда после предлога могут идти самые неожиданные вещи: \"я что, должен ехать на, черт побери, северный полюс?\". Во-вторых, неплохо бы учитывать отсутствие пунктуации (конечно, в норме, как нам кажется, предлог обязательно требует зависимое, но! \"да иди ты на!\") Эти штуки можно отсеять, если просто учитывать только заранее определенные падежи, а не считать все, какие встретились (так и None можно огрести). \n",
    "\n",
    "Если будете использовать RNNMorph, возможно, понадобится регулярное выражение и немного терпения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aeb719a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pymorphy2 nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a306018-c6cd-4254-ae7e-0c43f397c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "prepositions = ['в', 'на', 'в', 'за', 'о', 'по', 'про', 'сквозь', 'через', 'без', 'ввиду', 'вдоль', 'вместо', 'вне', 'внутри', 'внутрь', 'возле', 'впереди', 'вследствие', 'вокруг', 'для', 'до', 'из', 'из-за', 'из-под', 'кроме', 'между', 'мимо', 'напротив', 'насчет', 'около', 'от', 'относительно', 'помимо', 'посреди', 'посредством', 'подле', 'позади', 'путём', 'ради', 'с', 'сверх', 'среди', 'у', 'чтоб', 'чтобы']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c6b3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import nltk\n",
    "from pymorphy2 import MorphAnalyzer'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94496ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0db6194",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def analyze_text(text):\n",
    "  sentences = nltk.sent_tokenize(text)\n",
    "  word_cases = {}\n",
    "  for sentence in sentences:\n",
    "      words = nltk.word_tokenize(sentence)\n",
    "      for word in words:\n",
    "          parsed_word = morph.parse(word)[0]\n",
    "          word_form = parsed_word.word\n",
    "          word_case = parsed_word.tag.case\n",
    "          if word_form in prepositions:\n",
    "              if word_case not in word_cases:\n",
    "                word_cases[word_case] = 0\n",
    "              word_cases[word_case] += 1\n",
    "  return word_cases'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "94d8116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''with open(r'C:\\Users\\HP\\Documents\\python\\IntroCL2023\\04-NLPtools\\sample3.txt', 'r', encoding='utf-8') as file:\n",
    "   text = file.read()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a13152a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''word_cases = analyze_text(text)\n",
    "for case, count in word_cases.items():\n",
    "  if case is not None:\n",
    "      print(f\"Падеж {case}: {count} раз\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa99929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8e071151",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "159df432",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.split()\n",
    "analyzed_words = [morph.parse(word)[0] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba62b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_after_prepositions = ['CASE' in word.tag.grammemes for word in analyzed_words if word.tag.POS == \"PREP\" and word.word in prepositions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "051cbf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_counts = Counter(cases_after_prepositions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "76ec627f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({False: 58})\n"
     ]
    }
   ],
   "source": [
    "print(case_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f8c885-9e8f-4bbc-a0ab-ac801af64245",
   "metadata": {},
   "source": [
    "#### Задача 3 (5 баллов). \n",
    "\n",
    "Представим, что у вас есть файл с разборами conllu (можете взять любой, например, [тут](https://github.com/dialogue-evaluation/GramEval2020)). Нужно просмотреть все примеры предложений с тегом dislocated и тегом discourse: напишите скрипт, который будет читать файл, находить все такие предложения и печатать: 1) сам текст предложения 2) слово, имеющее искомый тег. Если тег не был найден в файле, нужно об этом сообщить. Постарайтесь оформить вывод таким образом, чтобы это было удобно читать. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07ede91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting conllu\n",
      "  Obtaining dependency information for conllu from https://files.pythonhosted.org/packages/ce/3f/70a1dc5bc536755ec082b806594598a10cfffaf0de978f51d4e0e4fdfa47/conllu-4.5.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: conllu\n",
      "Successfully installed conllu-4.5.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\hp\\appdata\\local\\programs\\python\\python38\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -otobuf (c:\\users\\hp\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f165dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef478943-1842-4dfe-98dc-327860a907bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def find_sentences(file_path):\n",
    "   with open(file_path, 'r') as file:\n",
    "       data = file.read()\n",
    "\n",
    "   sentences = parse(data)\n",
    "   found_sentences = []\n",
    "\n",
    "   for sentence in sentences:\n",
    "       for token in sentence:\n",
    "           if 'dislocated' in token['deps'] or 'discourse' in token['deps']:\n",
    "               found_sentences.append((sentence, token))\n",
    "\n",
    "   if not found_sentences:\n",
    "       print(\"Теги не найдены\")\n",
    "   else:\n",
    "       for sentence, token in found_sentences:\n",
    "           print(f\"Предложение: {' '.join(token['form'] for token in sentence)}\")\n",
    "           print(f\"Слово с тегом: {token['form']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbfcb404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Теги 'dislocated' и 'discourse' не найдены в файле.\n"
     ]
    }
   ],
   "source": [
    "find_sentences(r'C:\\Users\\HP\\Documents\\python\\IntroCL2023\\04-NLPtools\\ConluuFile.conllu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd0db77-e073-40ff-8c7d-3512325ce991",
   "metadata": {},
   "source": [
    "#### Задача 4 (5 баллов).\n",
    "\n",
    "Возьмите любой достаточно длинный (лучше новостной) текст. Любым известным инструментом извлеките именованные сущности из этого текста и выведите их списком по категориям (т.е. персоны вместе, локации вместе, организации вместе). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install spacy\\n!python -m spacy download ru_core_news_sm'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''!pip install spacy\n",
    "!python -m spacy download ru_core_news_sm'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b0ff0b9-6bca-42a7-baea-f2c856e6059f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "nlp = spacy.load('ru_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "В ночь с 25 на 26 мая сего года группой из двух-трех бандитов осуществлено вооруженное нападение с использованием взрывного устройства на блок-пост милиции при въезде в город Ханабад Андижанской области. В ходе завязавшейся перестрелки получили ранения один сотрудник милиции и один из нападавших бандитов. Вследствие полученного отпора под покровом ночи бандитам удалось скрыться.\n",
    "\n",
    "В 13:45-14:00 26 мая сего года на улице Фитрата в городе Андижан неизвестным мужчиной произведен самоподрыв, в результате которого один сотрудник милиции погиб и несколько граждан получили ранения.\n",
    "\n",
    "По предварительным данным, группа бандитов проникла на территорию Андижанской области из соседнего Кыргызстана. В настоящее время в городе Андижан создан и работает оперативный штаб, в состав которого вошли представители всех силовых структур республики. Ведутся оперативно-поисковые мероприятия и следственные действия\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = [ent for ent in entities if ent[1] == 'PERSON'] # сортируем категории\n",
    "locations = [ent for ent in entities if ent[1] == 'LOC']\n",
    "organizations = [ent for ent in entities if ent[1] == 'ORG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persons: []\n",
      "Locations: [('Ханабад', 'LOC'), ('Андижанской области', 'LOC'), ('Фитрата', 'LOC'), ('Андижан', 'LOC'), ('Андижанской области', 'LOC'), ('Кыргызстана', 'LOC'), ('Андижан', 'LOC')]\n",
      "Organizations: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Persons:\", persons)\n",
    "print(\"Locations:\", locations)\n",
    "print(\"Organizations:\", organizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4762e8d-baca-4d22-b6f3-3866fee70a7d",
   "metadata": {},
   "source": [
    "#### Задача на бонусные 5 баллов:\n",
    "\n",
    "Сравните качество несколькиз разных морфопарсеров для любого языка, где их больше одного. Разберите этими морфопарсерами один и тот же текст, если они все разбирают в UD, можете вывести автоматически расхождения, в противном случае просмотрите глазами на наличие ошибок (текст, конечно, слишком большой лучше не брать). \n",
    "\n",
    "Без письменных выводов-комментариев не засчитывается. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "89279821-5d4f-428e-b61c-33028a3d1374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'analysis': [{'lex': 'белка', 'gr': 'S,жен,од=им,ед'}], 'text': 'Белка'}, {'text': ' '}, {'analysis': [{'lex': 'спускаться', 'gr': 'V,нп=прош,ед,изъяв,жен,сов'}], 'text': 'спустилась'}, {'text': ' '}, {'analysis': [{'lex': 'по', 'gr': 'PR='}], 'text': 'по'}, {'text': ' '}, {'analysis': [{'lex': 'ствол', 'gr': 'S,муж,неод=дат,ед'}], 'text': 'стволу'}, {'text': ' '}, {'analysis': [{'lex': 'сосна', 'gr': 'S,жен,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'сосны'}, {'text': ' , '}, {'analysis': [{'lex': 'у', 'gr': 'PR='}], 'text': 'у'}, {'text': ' '}, {'analysis': [{'lex': 'самый', 'gr': 'APRO=(пр,мн|род,мн|вин,мн,од)'}], 'text': 'самых'}, {'text': ' '}, {'analysis': [{'lex': 'мой', 'gr': 'APRO=(пр,мн|род,мн|вин,мн,од)'}], 'text': 'моих'}, {'text': ' '}, {'analysis': [{'lex': 'нога', 'gr': 'S,жен,неод=род,мн'}], 'text': 'ног'}, {'text': ' '}, {'analysis': [{'lex': 'сковыривать', 'gr': 'V,пе=прош,ед,изъяв,жен,сов'}], 'text': 'сковырнула'}, {'text': ' '}, {'analysis': [{'lex': 'сыроежка', 'gr': 'S,жен,неод=вин,ед'}], 'text': 'сыроежку'}, {'text': ' '}, {'analysis': [{'lex': 'и', 'gr': 'CONJ='}], 'text': 'и'}, {'text': ' '}, {'analysis': [{'lex': 'приниматься', 'gr': 'V,нп=прош,ед,изъяв,жен,сов'}], 'text': 'принялась'}, {'text': ' '}, {'analysis': [{'lex': 'ее', 'gr': 'APRO=(пр,мн|дат,мн|род,мн|твор,мн|им,мн|им,ед,жен|вин,ед,муж,од|род,ед,муж|род,ед,сред|вин,ед,сред|им,ед,сред|пр,ед,жен|дат,ед,жен|род,ед,жен|твор,ед,жен|пр,ед,муж|пр,ед,сред|дат,ед,муж|дат,ед,сред|вин,ед,жен|вин,мн,неод|вин,ед,муж,неод|им,ед,муж|твор,ед,муж|твор,ед,сред|вин,мн,од|пр,мн,сред|вин,мн,сред|дат,мн,сред|род,мн,сред|твор,мн,сред|им,мн,сред|пр,мн,муж|вин,мн,муж|вин,ед,муж|дат,мн,муж)'}], 'text': 'её'}, {'text': ' '}, {'analysis': [{'lex': 'быть', 'gr': 'V,нп=(непрош,ед,изъяв,3-л,несов|непрош,мн,изъяв,3-л,несов|наст,мн,изъяв,1-л,несов|наст,ед,изъяв,1-л,несов|наст,мн,изъяв,2-л,несов|наст,ед,изъяв,2-л,несов|наст,мн,изъяв,3-л,несов|наст,ед,изъяв,3-л,несов)'}], 'text': 'есть'}, {'text': ', '}, {'analysis': [{'lex': 'а', 'gr': 'CONJ='}], 'text': 'а'}, {'text': ' '}, {'analysis': [{'lex': 'гриб', 'gr': 'S,муж,неод=(вин,ед|им,ед)'}], 'text': 'гриб'}, {'text': ' '}, {'analysis': [{'lex': 'держать', 'gr': 'V,несов,пе=прош,ед,изъяв,жен'}], 'text': 'держала'}, {'text': ' '}, {'analysis': [{'lex': 'в', 'gr': 'PR='}], 'text': 'в'}, {'text': ' '}, {'analysis': [{'lex': 'передний', 'gr': 'A=(пр,мн,полн|вин,мн,полн,од|род,мн,полн)'}], 'text': 'передних'}, {'text': ' '}, {'analysis': [{'lex': 'лапа', 'gr': 'S,жен,неод=пр,мн'}], 'text': 'лапах'}, {'text': '. '}, {'analysis': [{'lex': 'ее', 'gr': 'APRO=(пр,мн|дат,мн|род,мн|твор,мн|им,мн|им,ед,жен|вин,ед,муж,од|род,ед,муж|род,ед,сред|вин,ед,сред|им,ед,сред|пр,ед,жен|дат,ед,жен|род,ед,жен|твор,ед,жен|пр,ед,муж|пр,ед,сред|дат,ед,муж|дат,ед,сред|вин,ед,жен|вин,мн,неод|вин,ед,муж,неод|им,ед,муж|твор,ед,муж|твор,ед,сред|вин,мн,од|пр,мн,сред|вин,мн,сред|дат,мн,сред|род,мн,сред|твор,мн,сред|им,мн,сред|пр,мн,муж|вин,мн,муж|вин,ед,муж|дат,мн,муж)'}], 'text': 'Её'}, {'text': ' '}, {'analysis': [{'lex': 'щека', 'gr': 'S,жен,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'щёки'}, {'text': ' '}, {'analysis': [{'lex': 'быстро', 'gr': 'ADV='}], 'text': 'быстро'}, {'text': ' '}, {'analysis': [{'lex': 'двигаться', 'gr': 'V,несов,нп=прош,мн,изъяв'}], 'text': 'двигались'}, {'text': ', '}, {'analysis': [{'lex': 'и', 'gr': 'CONJ='}], 'text': 'и'}, {'text': ' '}, {'analysis': [{'lex': 'на', 'gr': 'PR='}], 'text': 'на'}, {'text': ' '}, {'analysis': [{'lex': 'земля', 'gr': 'S,жен,неод=вин,ед'}], 'text': 'землю'}, {'text': ' '}, {'analysis': [{'lex': 'сыпаться', 'gr': 'V,несов,нп=прош,мн,изъяв'}], 'text': 'сыпались'}, {'text': ' '}, {'analysis': [{'lex': 'крошка', 'gr': 'S,жен,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'крошки'}, {'text': ' '}, {'analysis': [{'lex': 'гриб', 'gr': 'S,муж,неод=род,ед'}], 'text': 'гриба'}, {'text': '. '}, {'analysis': [{'lex': 'она', 'gr': 'SPRO,ед,3-л,жен=им'}], 'text': 'Она'}, {'text': ' '}, {'analysis': [{'lex': 'съедать', 'gr': 'V=прош,ед,изъяв,жен,сов'}], 'text': 'съела'}, {'text': ' '}, {'analysis': [{'lex': 'шляпка', 'gr': 'S,жен,неод=вин,ед'}], 'text': 'шляпку'}, {'text': ', '}, {'analysis': [{'lex': 'но', 'gr': 'CONJ='}], 'text': 'но'}, {'text': ' '}, {'analysis': [{'lex': 'корешок', 'gr': 'S,муж,неод=(вин,ед|им,ед)'}], 'text': 'корешок'}, {'text': ' '}, {'analysis': [{'lex': 'она', 'gr': 'SPRO,ед,3-л,жен=(дат|твор)'}], 'text': 'ей'}, {'text': ' '}, {'analysis': [{'lex': 'не', 'gr': 'PART='}], 'text': 'не'}, {'text': ' '}, {'analysis': [{'lex': 'понравиться', 'gr': 'V,сов,нп=прош,ед,изъяв,муж'}], 'text': 'понравился'}, {'text': ', '}, {'analysis': [{'lex': 'и', 'gr': 'CONJ='}], 'text': 'и'}, {'text': ' '}, {'analysis': [{'lex': 'она', 'gr': 'SPRO,ед,3-л,жен=им'}], 'text': 'она'}, {'text': ' '}, {'analysis': [{'lex': 'переходить', 'gr': 'V,пе=прош,ед,изъяв,жен,сов'}], 'text': 'перешла'}, {'text': ' '}, {'analysis': [{'lex': 'на', 'gr': 'PR='}], 'text': 'на'}, {'text': ' '}, {'analysis': [{'lex': 'другой', 'gr': 'APRO=(вин,ед,сред|им,ед,сред)'}], 'text': 'другое'}, {'text': ' '}, {'analysis': [{'lex': 'место', 'gr': 'S,сред,неод=(вин,ед|им,ед)'}], 'text': 'место'}, {'text': '.'}, {'text': '\\n'}]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import pymystem3\n",
    "\n",
    "stemmer = pymystem3.Mystem()\n",
    "text = \"Белка спустилась по стволу сосны , у самых моих ног сковырнула сыроежку и принялась её есть, а гриб держала в передних лапах. Её щёки быстро двигались, и на землю сыпались крошки гриба. Она съела шляпку, но корешок ей не понравился, и она перешла на другое место.\"\n",
    "analyzed_text = stemmer.analyze(text)\n",
    "print(analyzed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5d7fbfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parse(word='вбелка', tag=OpencorporaTag('NOUN,anim,femn sing,nomn'), normal_form='вбелка', score=0.25806451612903225, methods_stack=((DictionaryAnalyzer(), 'белка', 40, 0), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'в'))), Parse(word='спустилась', tag=OpencorporaTag('VERB,perf,intr femn,sing,past,indc'), normal_form='спуститься', score=1.0, methods_stack=((DictionaryAnalyzer(), 'спустилась', 799, 2),)), Parse(word='по', tag=OpencorporaTag('PREP'), normal_form='по', score=0.998387, methods_stack=((DictionaryAnalyzer(), 'по', 24, 0),)), Parse(word='стволу', tag=OpencorporaTag('NOUN,inan,masc sing,datv'), normal_form='ствол', score=1.0, methods_stack=((DictionaryAnalyzer(), 'стволу', 34, 2),)), Parse(word='сосны', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='сосна', score=0.5, methods_stack=((DictionaryAnalyzer(), 'сосны', 1271, 7),)), Parse(word=',', tag=OpencorporaTag('PNCT'), normal_form=',', score=1.0, methods_stack=((PunctuationAnalyzer(score=0.9), ','),)), Parse(word='у', tag=OpencorporaTag('PREP'), normal_form='у', score=0.9959, methods_stack=((DictionaryAnalyzer(), 'у', 24, 0),)), Parse(word='самых', tag=OpencorporaTag('ADJF,Apro plur,gent'), normal_form='самый', score=0.870967, methods_stack=((DictionaryAnalyzer(), 'самых', 2826, 20),)), Parse(word='моих', tag=OpencorporaTag('ADJF,Apro plur,gent'), normal_form='мой', score=0.716666, methods_stack=((DictionaryAnalyzer(), 'моих', 2049, 20),)), Parse(word='ног', tag=OpencorporaTag('NOUN,inan,femn plur,gent'), normal_form='нога', score=1.0, methods_stack=((DictionaryAnalyzer(), 'ног', 44, 8),)), Parse(word='сковырнула', tag=OpencorporaTag('VERB,perf,tran femn,sing,past,indc'), normal_form='сковырнуть', score=1.0, methods_stack=((DictionaryAnalyzer(), 'сковырнула', 758, 2),)), Parse(word='сыроежку', tag=OpencorporaTag('NOUN,inan,femn sing,accs'), normal_form='сыроежка', score=1.0, methods_stack=((DictionaryAnalyzer(), 'сыроежку', 9, 3),)), Parse(word='и', tag=OpencorporaTag('CONJ'), normal_form='и', score=0.998263, methods_stack=((DictionaryAnalyzer(), 'и', 20, 0),)), Parse(word='принялась', tag=OpencorporaTag('VERB,perf,intr femn,sing,past,indc'), normal_form='приняться', score=1.0, methods_stack=((DictionaryAnalyzer(), 'принялась', 2670, 2),)), Parse(word='её', tag=OpencorporaTag('ADJF,Fixd,Apro,Anph neut,sing,gent'), normal_form='её', score=0.074074, methods_stack=((DictionaryAnalyzer(), 'её', 1440, 13),)), Parse(word='есть,', tag=OpencorporaTag('UNKN'), normal_form='есть,', score=1.0, methods_stack=((UnknAnalyzer(), 'есть,'),)), Parse(word='а', tag=OpencorporaTag('CONJ'), normal_form='а', score=0.997155, methods_stack=((DictionaryAnalyzer(), 'а', 20, 0),)), Parse(word='гриб', tag=OpencorporaTag('NOUN,inan,masc sing,nomn'), normal_form='гриб', score=0.5, methods_stack=((DictionaryAnalyzer(), 'гриб', 34, 0),)), Parse(word='держала', tag=OpencorporaTag('VERB,impf,tran femn,sing,past,indc'), normal_form='держать', score=1.0, methods_stack=((DictionaryAnalyzer(), 'держала', 1333, 8),)), Parse(word='в', tag=OpencorporaTag('PREP'), normal_form='в', score=0.999327, methods_stack=((DictionaryAnalyzer(), 'в', 393, 0),)), Parse(word='передних', tag=OpencorporaTag('ADJF plur,gent'), normal_form='передний', score=0.363636, methods_stack=((DictionaryAnalyzer(), 'передних', 493, 21),)), Parse(word='лапах.', tag=OpencorporaTag('UNKN'), normal_form='лапах.', score=1.0, methods_stack=((UnknAnalyzer(), 'лапах.'),)), Parse(word='её', tag=OpencorporaTag('ADJF,Fixd,Apro,Anph neut,sing,gent'), normal_form='её', score=0.074074, methods_stack=((DictionaryAnalyzer(), 'её', 1440, 13),)), Parse(word='щёки', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='щека', score=0.666666, methods_stack=((DictionaryAnalyzer(), 'щёки', 3232, 11),)), Parse(word='быстро', tag=OpencorporaTag('ADVB'), normal_form='быстро', score=0.979591, methods_stack=((DictionaryAnalyzer(), 'быстро', 3, 0),)), Parse(word='двигались,', tag=OpencorporaTag('UNKN'), normal_form='двигались,', score=1.0, methods_stack=((UnknAnalyzer(), 'двигались,'),)), Parse(word='и', tag=OpencorporaTag('CONJ'), normal_form='и', score=0.998263, methods_stack=((DictionaryAnalyzer(), 'и', 20, 0),)), Parse(word='на', tag=OpencorporaTag('PREP'), normal_form='на', score=0.998961, methods_stack=((DictionaryAnalyzer(), 'на', 24, 0),)), Parse(word='землю', tag=OpencorporaTag('NOUN,inan,femn sing,accs'), normal_form='земля', score=1.0, methods_stack=((DictionaryAnalyzer(), 'землю', 1670, 3),)), Parse(word='сыпались', tag=OpencorporaTag('VERB,impf,intr plur,past,indc'), normal_form='сыпаться', score=1.0, methods_stack=((DictionaryAnalyzer(), 'сыпались', 2987, 15),)), Parse(word='крошки', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='крошка', score=0.285714, methods_stack=((DictionaryAnalyzer(), 'крошки', 9, 10),)), Parse(word='гриба.', tag=OpencorporaTag('UNKN'), normal_form='гриба.', score=1.0, methods_stack=((UnknAnalyzer(), 'гриба.'),)), Parse(word='она', tag=OpencorporaTag('NPRO,femn,3per,Anph sing,nomn'), normal_form='она', score=1.0, methods_stack=((DictionaryAnalyzer(), 'она', 2305, 0),)), Parse(word='съела', tag=OpencorporaTag('VERB,perf,tran femn,sing,past,indc'), normal_form='съесть', score=1.0, methods_stack=((DictionaryAnalyzer(), 'съела', 1064, 2),)), Parse(word='шляпку,', tag=OpencorporaTag('UNKN'), normal_form='шляпку,', score=1.0, methods_stack=((UnknAnalyzer(), 'шляпку,'),)), Parse(word='но', tag=OpencorporaTag('CONJ'), normal_form='но', score=0.999185, methods_stack=((DictionaryAnalyzer(), 'но', 20, 0),)), Parse(word='корешок', tag=OpencorporaTag('NOUN,inan,masc sing,accs'), normal_form='корешок', score=0.8, methods_stack=((DictionaryAnalyzer(), 'корешок', 141, 3),)), Parse(word='ей', tag=OpencorporaTag('NPRO,femn,3per,Anph sing,datv'), normal_form='она', score=0.97093, methods_stack=((DictionaryAnalyzer(), 'ей', 2305, 4),)), Parse(word='не', tag=OpencorporaTag('PRCL'), normal_form='не', score=1.0, methods_stack=((DictionaryAnalyzer(), 'не', 22, 0),)), Parse(word='понравился,', tag=OpencorporaTag('UNKN'), normal_form='понравился,', score=1.0, methods_stack=((UnknAnalyzer(), 'понравился,'),)), Parse(word='и', tag=OpencorporaTag('CONJ'), normal_form='и', score=0.998263, methods_stack=((DictionaryAnalyzer(), 'и', 20, 0),)), Parse(word='она', tag=OpencorporaTag('NPRO,femn,3per,Anph sing,nomn'), normal_form='она', score=1.0, methods_stack=((DictionaryAnalyzer(), 'она', 2305, 0),)), Parse(word='перешла', tag=OpencorporaTag('VERB,perf,tran femn,sing,past,indc'), normal_form='перейти', score=1.0, methods_stack=((DictionaryAnalyzer(), 'перешла', 2255, 2),)), Parse(word='на', tag=OpencorporaTag('PREP'), normal_form='на', score=0.998961, methods_stack=((DictionaryAnalyzer(), 'на', 24, 0),)), Parse(word='другое', tag=OpencorporaTag('ADJF,Subx,Apro neut,sing,nomn'), normal_form='другой', score=0.588235, methods_stack=((DictionaryAnalyzer(), 'другое', 1424, 14),)), Parse(word='место.', tag=OpencorporaTag('UNKN'), normal_form='место.', score=1.0, methods_stack=((UnknAnalyzer(), 'место.'),))]\n"
     ]
    }
   ],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "text = \"ВБелка спустилась по стволу сосны , у самых моих ног сковырнула сыроежку и принялась её есть, а гриб держала в передних лапах. Её щёки быстро двигались, и на землю сыпались крошки гриба. Она съела шляпку, но корешок ей не понравился, и она перешла на другое место.\"\n",
    "analyzed_text = [morph.parse(word)[0] for word in text.split()]\n",
    "print(analyzed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a27e1",
   "metadata": {},
   "source": [
    "Mystem анализирует слово \"белка\" как существительное \"белка\". Pymorphy2 анализирует это слово как \"вбелка\", что является ошибкой.\n",
    "\n",
    "Mystem анализирует слово \"спустилась\" как глагол \"спуститься\" в прошедшем времени, в то время как Pymorphy2 анализирует это слово как \"спустилась\".\n",
    "\n",
    "Mystem анализирует слово \"по\" как предлог, а Pymorphy2 анализирует это слово как \"по\".\n",
    "\n",
    "Mystem анализирует слово \"стволу\" как существительное \"ствол\" в дательном падеже, в то время как Pymorphy2 анализирует это слово как \"стволу\".\n",
    "\n",
    "Mystem анализирует слово \"сосны\" как существительное \"сосна\" в множественном числе, а Pymorphy2 анализирует это слово как \"сосны\".\n",
    "\n",
    "Mystem анализирует слово \"у\" как предлог, а Pymorphy2 анализирует это слово как \"у\".\n",
    "\n",
    "Mystem анализирует слово \"самых\" как прилагательное \"самый\" в родительном падеже, в то время как Pymorphy2 анализирует это слово как \"самых\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
